{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "Gkca6u_Ty3ay",
    "outputId": "1bef6164-03ae-4ece-cb9e-bcafbce34558"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0810 10:00:36.261825 140324080813952 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "W0810 10:00:36.263410 140324080813952 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "W0810 10:00:36.275182 140324080813952 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W0810 10:00:36.280717 140324080813952 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W0810 10:00:36.288332 140324080813952 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "W0810 10:00:36.290963 140324080813952 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Activation, MaxPooling2D, Dropout, Flatten, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "import tflearn.datasets.oxflower17 as oxflower17\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6MP3mWf1y3bA"
   },
   "outputs": [],
   "source": [
    "def vgg19_model(img_shape=(224, 224, 3), classes = 1000):\n",
    "    vgg19 = Sequential()\n",
    "\n",
    "    # Convolutional Layer 1\n",
    "    # Padding is same to keep the the spatial resolution same. (For 3x3 filter, padding = 1)\n",
    "    vgg19.add(Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005), input_shape=img_shape))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 2\n",
    "    vgg19.add(Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Maxpooling Layer 1\n",
    "    vgg19.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Convolutional Layer 3\n",
    "    vgg19.add(Conv2D(filters=128, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 4\n",
    "    vgg19.add(Conv2D(filters=128, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Maxpooling Layer 2\n",
    "    vgg19.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Convolutional Layer 5\n",
    "    vgg19.add(Conv2D(filters=256, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 6\n",
    "    vgg19.add(Conv2D(filters=256, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 7\n",
    "    vgg19.add(Conv2D(filters=256, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 8\n",
    "    vgg19.add(Conv2D(filters=256, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Maxpooling Layer 3\n",
    "    vgg19.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Convolutional Layer 9\n",
    "    vgg19.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 10\n",
    "    vgg19.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 11\n",
    "    vgg19.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 12\n",
    "    vgg19.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Maxpooling Layer 4\n",
    "    vgg19.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Convolutional Layer 13\n",
    "    vgg19.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 14\n",
    "    vgg19.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 15\n",
    "    vgg19.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 16\n",
    "    vgg19.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "    \n",
    "    # Maxpooling Layer 5\n",
    "    vgg19.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "    # Dense Layer 1\n",
    "    vgg19.add(Flatten())\n",
    "    vgg19.add(Dropout(0.5))\n",
    "    vgg19.add(Dense(4096, kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "\n",
    "    # Dense Layer 2\n",
    "    vgg19.add(Dropout(0.5))\n",
    "    vgg19.add(Dense(4096, kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('relu'))\n",
    "\n",
    "    # Dense Layer 3\n",
    "    vgg19.add(Dense(classes, kernel_regularizer=l2(0.0005)))\n",
    "    vgg19.add(Activation('softmax'))\n",
    "    \n",
    "    return vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9RVvG8h_y3bG"
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "X, Y = oxflower17.load_data(one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "O84nU89wy3bM",
    "outputId": "0a3e4a46-a6c4-4594-ac3f-724c2df5e0c7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X's shape:  (1360, 224, 224, 3)\n",
      "Y's shape:  (1360, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"X's shape: \", X.shape)\n",
    "print(\"Y's shape: \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "vk_ODuSby3bS",
    "outputId": "7f8a9d4c-1443-43b7-f32a-f5d1061fd0ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train's shape:  (1088, 224, 224, 3)\n",
      "Y_train's shape:  (1088, 17)\n",
      "X_test's shape:  (272, 224, 224, 3)\n",
      "Y_test's shape:  (272, 17)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "print(\"X_train's shape: \", X_train.shape)\n",
    "print(\"Y_train's shape: \", Y_train.shape)\n",
    "print(\"X_test's shape: \", X_test.shape)\n",
    "print(\"Y_test's shape: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BqhTJu6Jy3bY",
    "outputId": "3d8710a1-b47b-4007-bbc2-6ff9dd1ab489"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0810 10:00:37.848726 140324080813952 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0810 10:00:37.894550 140324080813952 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0810 10:00:38.263224 140324080813952 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 224, 224, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 224, 224, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 112, 112, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 112, 112, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 17)                69649     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 17)                0         \n",
      "=================================================================\n",
      "Total params: 139,639,889\n",
      "Trainable params: 139,639,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg19 = vgg19_model((224,224,3), 17)\n",
    "vgg19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "T4ZNiAy4y3bd",
    "outputId": "988dffc7-ef18-49ff-8895-244353b6e145"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0810 10:00:38.392172 140324080813952 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0810 10:00:38.731463 140324080813952 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 816 samples, validate on 272 samples\n",
      "Epoch 1/700\n",
      "816/816 [==============================] - 55s 68ms/step - loss: 10.9925 - acc: 0.0539 - val_loss: 10.9919 - val_acc: 0.0515\n",
      "Epoch 2/700\n",
      "816/816 [==============================] - 15s 19ms/step - loss: 10.9913 - acc: 0.0576 - val_loss: 10.9908 - val_acc: 0.0478\n",
      "Epoch 3/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 10.9900 - acc: 0.0711 - val_loss: 10.9898 - val_acc: 0.0551\n",
      "Epoch 4/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 10.9888 - acc: 0.0637 - val_loss: 10.9888 - val_acc: 0.0331\n",
      "Epoch 5/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 10.9875 - acc: 0.0625 - val_loss: 10.9878 - val_acc: 0.0331\n",
      "Epoch 6/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 10.9864 - acc: 0.0588 - val_loss: 10.9868 - val_acc: 0.0331\n",
      "Epoch 7/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9851 - acc: 0.0686 - val_loss: 10.9858 - val_acc: 0.0331\n",
      "Epoch 8/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9839 - acc: 0.0662 - val_loss: 10.9848 - val_acc: 0.0331\n",
      "Epoch 9/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9828 - acc: 0.0637 - val_loss: 10.9839 - val_acc: 0.0331\n",
      "Epoch 10/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9814 - acc: 0.0662 - val_loss: 10.9828 - val_acc: 0.0331\n",
      "Epoch 11/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9804 - acc: 0.0699 - val_loss: 10.9819 - val_acc: 0.0331\n",
      "Epoch 12/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9791 - acc: 0.0686 - val_loss: 10.9809 - val_acc: 0.0331\n",
      "Epoch 13/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9779 - acc: 0.0686 - val_loss: 10.9799 - val_acc: 0.0331\n",
      "Epoch 14/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9766 - acc: 0.0686 - val_loss: 10.9789 - val_acc: 0.0331\n",
      "Epoch 15/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9755 - acc: 0.0613 - val_loss: 10.9779 - val_acc: 0.0331\n",
      "Epoch 16/700\n",
      "816/816 [==============================] - 17s 21ms/step - loss: 10.9742 - acc: 0.0760 - val_loss: 10.9769 - val_acc: 0.0331\n",
      "Epoch 17/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9730 - acc: 0.0650 - val_loss: 10.9758 - val_acc: 0.0331\n",
      "Epoch 18/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9719 - acc: 0.0686 - val_loss: 10.9748 - val_acc: 0.0331\n",
      "Epoch 19/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9706 - acc: 0.0723 - val_loss: 10.9738 - val_acc: 0.0331\n",
      "Epoch 20/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9694 - acc: 0.0613 - val_loss: 10.9728 - val_acc: 0.0331\n",
      "Epoch 21/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9682 - acc: 0.0760 - val_loss: 10.9718 - val_acc: 0.0331\n",
      "Epoch 22/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9671 - acc: 0.0735 - val_loss: 10.9708 - val_acc: 0.0331\n",
      "Epoch 23/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9660 - acc: 0.0699 - val_loss: 10.9698 - val_acc: 0.0331\n",
      "Epoch 24/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9648 - acc: 0.0637 - val_loss: 10.9688 - val_acc: 0.0331\n",
      "Epoch 25/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9634 - acc: 0.0686 - val_loss: 10.9678 - val_acc: 0.0331\n",
      "Epoch 26/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9621 - acc: 0.0650 - val_loss: 10.9668 - val_acc: 0.0331\n",
      "Epoch 27/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9610 - acc: 0.0637 - val_loss: 10.9658 - val_acc: 0.0331\n",
      "Epoch 28/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9598 - acc: 0.0637 - val_loss: 10.9648 - val_acc: 0.0331\n",
      "Epoch 29/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9585 - acc: 0.0686 - val_loss: 10.9638 - val_acc: 0.0331\n",
      "Epoch 30/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9576 - acc: 0.0662 - val_loss: 10.9628 - val_acc: 0.0331\n",
      "Epoch 31/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9563 - acc: 0.0637 - val_loss: 10.9617 - val_acc: 0.0331\n",
      "Epoch 32/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9551 - acc: 0.0674 - val_loss: 10.9607 - val_acc: 0.0331\n",
      "Epoch 33/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9541 - acc: 0.0625 - val_loss: 10.9598 - val_acc: 0.0331\n",
      "Epoch 34/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9528 - acc: 0.0613 - val_loss: 10.9588 - val_acc: 0.0331\n",
      "Epoch 35/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9515 - acc: 0.0662 - val_loss: 10.9578 - val_acc: 0.0331\n",
      "Epoch 36/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9503 - acc: 0.0674 - val_loss: 10.9569 - val_acc: 0.0331\n",
      "Epoch 37/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9492 - acc: 0.0699 - val_loss: 10.9558 - val_acc: 0.0331\n",
      "Epoch 38/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9479 - acc: 0.0674 - val_loss: 10.9549 - val_acc: 0.0331\n",
      "Epoch 39/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9469 - acc: 0.0686 - val_loss: 10.9539 - val_acc: 0.0331\n",
      "Epoch 40/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9457 - acc: 0.0735 - val_loss: 10.9529 - val_acc: 0.0331\n",
      "Epoch 41/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9446 - acc: 0.0699 - val_loss: 10.9519 - val_acc: 0.0331\n",
      "Epoch 42/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9431 - acc: 0.0711 - val_loss: 10.9509 - val_acc: 0.0331\n",
      "Epoch 43/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9422 - acc: 0.0699 - val_loss: 10.9498 - val_acc: 0.0331\n",
      "Epoch 44/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9410 - acc: 0.0674 - val_loss: 10.9488 - val_acc: 0.0331\n",
      "Epoch 45/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9398 - acc: 0.0662 - val_loss: 10.9478 - val_acc: 0.0331\n",
      "Epoch 46/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9384 - acc: 0.0662 - val_loss: 10.9468 - val_acc: 0.0331\n",
      "Epoch 47/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9374 - acc: 0.0686 - val_loss: 10.9458 - val_acc: 0.0331\n",
      "Epoch 48/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9361 - acc: 0.0674 - val_loss: 10.9447 - val_acc: 0.0331\n",
      "Epoch 49/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9351 - acc: 0.0686 - val_loss: 10.9437 - val_acc: 0.0331\n",
      "Epoch 50/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9337 - acc: 0.0711 - val_loss: 10.9427 - val_acc: 0.0331\n",
      "Epoch 51/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9328 - acc: 0.0674 - val_loss: 10.9417 - val_acc: 0.0331\n",
      "Epoch 52/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9317 - acc: 0.0662 - val_loss: 10.9407 - val_acc: 0.0331\n",
      "Epoch 53/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9302 - acc: 0.0650 - val_loss: 10.9397 - val_acc: 0.0331\n",
      "Epoch 54/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9293 - acc: 0.0699 - val_loss: 10.9387 - val_acc: 0.0331\n",
      "Epoch 55/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9279 - acc: 0.0686 - val_loss: 10.9377 - val_acc: 0.0331\n",
      "Epoch 56/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9269 - acc: 0.0686 - val_loss: 10.9367 - val_acc: 0.0331\n",
      "Epoch 57/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9257 - acc: 0.0662 - val_loss: 10.9357 - val_acc: 0.0331\n",
      "Epoch 58/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9247 - acc: 0.0686 - val_loss: 10.9347 - val_acc: 0.0331\n",
      "Epoch 59/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9235 - acc: 0.0686 - val_loss: 10.9336 - val_acc: 0.0331\n",
      "Epoch 60/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9225 - acc: 0.0674 - val_loss: 10.9326 - val_acc: 0.0331\n",
      "Epoch 61/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9212 - acc: 0.0674 - val_loss: 10.9316 - val_acc: 0.0331\n",
      "Epoch 62/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9199 - acc: 0.0699 - val_loss: 10.9306 - val_acc: 0.0331\n",
      "Epoch 63/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9189 - acc: 0.0686 - val_loss: 10.9295 - val_acc: 0.0331\n",
      "Epoch 64/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9174 - acc: 0.0662 - val_loss: 10.9285 - val_acc: 0.0331\n",
      "Epoch 65/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9163 - acc: 0.0674 - val_loss: 10.9275 - val_acc: 0.0331\n",
      "Epoch 66/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9151 - acc: 0.0650 - val_loss: 10.9264 - val_acc: 0.0331\n",
      "Epoch 67/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9143 - acc: 0.0674 - val_loss: 10.9254 - val_acc: 0.0331\n",
      "Epoch 68/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9132 - acc: 0.0674 - val_loss: 10.9244 - val_acc: 0.0331\n",
      "Epoch 69/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9117 - acc: 0.0686 - val_loss: 10.9233 - val_acc: 0.0331\n",
      "Epoch 70/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9108 - acc: 0.0637 - val_loss: 10.9223 - val_acc: 0.0331\n",
      "Epoch 71/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9096 - acc: 0.0686 - val_loss: 10.9213 - val_acc: 0.0331\n",
      "Epoch 72/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9083 - acc: 0.0674 - val_loss: 10.9203 - val_acc: 0.0331\n",
      "Epoch 73/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9072 - acc: 0.0662 - val_loss: 10.9193 - val_acc: 0.0331\n",
      "Epoch 74/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9059 - acc: 0.0699 - val_loss: 10.9183 - val_acc: 0.0331\n",
      "Epoch 75/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9048 - acc: 0.0674 - val_loss: 10.9173 - val_acc: 0.0331\n",
      "Epoch 76/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9038 - acc: 0.0662 - val_loss: 10.9163 - val_acc: 0.0331\n",
      "Epoch 77/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9024 - acc: 0.0674 - val_loss: 10.9153 - val_acc: 0.0331\n",
      "Epoch 78/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9013 - acc: 0.0699 - val_loss: 10.9143 - val_acc: 0.0331\n",
      "Epoch 79/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.9004 - acc: 0.0674 - val_loss: 10.9133 - val_acc: 0.0331\n",
      "Epoch 80/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8987 - acc: 0.0674 - val_loss: 10.9122 - val_acc: 0.0331\n",
      "Epoch 81/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8978 - acc: 0.0686 - val_loss: 10.9113 - val_acc: 0.0331\n",
      "Epoch 82/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8966 - acc: 0.0662 - val_loss: 10.9103 - val_acc: 0.0331\n",
      "Epoch 83/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8959 - acc: 0.0674 - val_loss: 10.9093 - val_acc: 0.0331\n",
      "Epoch 84/700\n",
      "816/816 [==============================] - 17s 20ms/step - loss: 10.8944 - acc: 0.0674 - val_loss: 10.9082 - val_acc: 0.0331\n",
      "Epoch 85/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8934 - acc: 0.0699 - val_loss: 10.9071 - val_acc: 0.0331\n",
      "Epoch 86/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8918 - acc: 0.0637 - val_loss: 10.9061 - val_acc: 0.0331\n",
      "Epoch 87/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8911 - acc: 0.0686 - val_loss: 10.9051 - val_acc: 0.0331\n",
      "Epoch 88/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8900 - acc: 0.0699 - val_loss: 10.9041 - val_acc: 0.0331\n",
      "Epoch 89/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8891 - acc: 0.0674 - val_loss: 10.9031 - val_acc: 0.0331\n",
      "Epoch 90/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8876 - acc: 0.0662 - val_loss: 10.9021 - val_acc: 0.0331\n",
      "Epoch 91/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8864 - acc: 0.0686 - val_loss: 10.9011 - val_acc: 0.0331\n",
      "Epoch 92/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8851 - acc: 0.0699 - val_loss: 10.8999 - val_acc: 0.0331\n",
      "Epoch 93/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8842 - acc: 0.0662 - val_loss: 10.8989 - val_acc: 0.0331\n",
      "Epoch 94/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8830 - acc: 0.0674 - val_loss: 10.8979 - val_acc: 0.0331\n",
      "Epoch 95/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8821 - acc: 0.0650 - val_loss: 10.8969 - val_acc: 0.0331\n",
      "Epoch 96/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8806 - acc: 0.0686 - val_loss: 10.8959 - val_acc: 0.0331\n",
      "Epoch 97/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8794 - acc: 0.0662 - val_loss: 10.8948 - val_acc: 0.0331\n",
      "Epoch 98/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8787 - acc: 0.0674 - val_loss: 10.8938 - val_acc: 0.0331\n",
      "Epoch 99/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8774 - acc: 0.0686 - val_loss: 10.8928 - val_acc: 0.0331\n",
      "Epoch 100/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8763 - acc: 0.0674 - val_loss: 10.8919 - val_acc: 0.0331\n",
      "Epoch 101/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8749 - acc: 0.0662 - val_loss: 10.8908 - val_acc: 0.0331\n",
      "Epoch 102/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8742 - acc: 0.0711 - val_loss: 10.8898 - val_acc: 0.0331\n",
      "Epoch 103/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8727 - acc: 0.0686 - val_loss: 10.8888 - val_acc: 0.0331\n",
      "Epoch 104/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8712 - acc: 0.0674 - val_loss: 10.8877 - val_acc: 0.0331\n",
      "Epoch 105/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8700 - acc: 0.0662 - val_loss: 10.8867 - val_acc: 0.0331\n",
      "Epoch 106/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8691 - acc: 0.0686 - val_loss: 10.8857 - val_acc: 0.0331\n",
      "Epoch 107/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8682 - acc: 0.0674 - val_loss: 10.8844 - val_acc: 0.0331\n",
      "Epoch 108/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8668 - acc: 0.0686 - val_loss: 10.8834 - val_acc: 0.0331\n",
      "Epoch 109/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8657 - acc: 0.0650 - val_loss: 10.8824 - val_acc: 0.0331\n",
      "Epoch 110/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8650 - acc: 0.0674 - val_loss: 10.8813 - val_acc: 0.0331\n",
      "Epoch 111/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8636 - acc: 0.0662 - val_loss: 10.8803 - val_acc: 0.0331\n",
      "Epoch 112/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8621 - acc: 0.0674 - val_loss: 10.8793 - val_acc: 0.0331\n",
      "Epoch 113/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8615 - acc: 0.0674 - val_loss: 10.8783 - val_acc: 0.0331\n",
      "Epoch 114/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8598 - acc: 0.0662 - val_loss: 10.8772 - val_acc: 0.0331\n",
      "Epoch 115/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8593 - acc: 0.0674 - val_loss: 10.8761 - val_acc: 0.0331\n",
      "Epoch 116/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8578 - acc: 0.0662 - val_loss: 10.8751 - val_acc: 0.0331\n",
      "Epoch 117/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8562 - acc: 0.0686 - val_loss: 10.8742 - val_acc: 0.0331\n",
      "Epoch 118/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8558 - acc: 0.0674 - val_loss: 10.8731 - val_acc: 0.0331\n",
      "Epoch 119/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8541 - acc: 0.0674 - val_loss: 10.8720 - val_acc: 0.0331\n",
      "Epoch 120/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8535 - acc: 0.0650 - val_loss: 10.8710 - val_acc: 0.0331\n",
      "Epoch 121/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8520 - acc: 0.0674 - val_loss: 10.8699 - val_acc: 0.0331\n",
      "Epoch 122/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8510 - acc: 0.0686 - val_loss: 10.8688 - val_acc: 0.0331\n",
      "Epoch 123/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8501 - acc: 0.0674 - val_loss: 10.8677 - val_acc: 0.0331\n",
      "Epoch 124/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8484 - acc: 0.0674 - val_loss: 10.8667 - val_acc: 0.0331\n",
      "Epoch 125/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8476 - acc: 0.0674 - val_loss: 10.8656 - val_acc: 0.0331\n",
      "Epoch 126/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8462 - acc: 0.0674 - val_loss: 10.8646 - val_acc: 0.0331\n",
      "Epoch 127/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8453 - acc: 0.0674 - val_loss: 10.8636 - val_acc: 0.0331\n",
      "Epoch 128/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8437 - acc: 0.0686 - val_loss: 10.8626 - val_acc: 0.0331\n",
      "Epoch 129/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8428 - acc: 0.0674 - val_loss: 10.8615 - val_acc: 0.0331\n",
      "Epoch 130/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8418 - acc: 0.0686 - val_loss: 10.8604 - val_acc: 0.0331\n",
      "Epoch 131/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8405 - acc: 0.0674 - val_loss: 10.8592 - val_acc: 0.0331\n",
      "Epoch 132/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8396 - acc: 0.0662 - val_loss: 10.8582 - val_acc: 0.0331\n",
      "Epoch 133/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8383 - acc: 0.0662 - val_loss: 10.8572 - val_acc: 0.0331\n",
      "Epoch 134/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8371 - acc: 0.0662 - val_loss: 10.8562 - val_acc: 0.0331\n",
      "Epoch 135/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8365 - acc: 0.0674 - val_loss: 10.8552 - val_acc: 0.0331\n",
      "Epoch 136/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8350 - acc: 0.0674 - val_loss: 10.8542 - val_acc: 0.0331\n",
      "Epoch 137/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8336 - acc: 0.0662 - val_loss: 10.8531 - val_acc: 0.0331\n",
      "Epoch 138/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8332 - acc: 0.0674 - val_loss: 10.8519 - val_acc: 0.0331\n",
      "Epoch 139/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8314 - acc: 0.0674 - val_loss: 10.8508 - val_acc: 0.0331\n",
      "Epoch 140/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8311 - acc: 0.0662 - val_loss: 10.8497 - val_acc: 0.0331\n",
      "Epoch 141/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8294 - acc: 0.0674 - val_loss: 10.8486 - val_acc: 0.0331\n",
      "Epoch 142/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8283 - acc: 0.0686 - val_loss: 10.8475 - val_acc: 0.0331\n",
      "Epoch 143/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8269 - acc: 0.0674 - val_loss: 10.8464 - val_acc: 0.0331\n",
      "Epoch 144/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8263 - acc: 0.0674 - val_loss: 10.8453 - val_acc: 0.0331\n",
      "Epoch 145/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8245 - acc: 0.0650 - val_loss: 10.8443 - val_acc: 0.0331\n",
      "Epoch 146/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8237 - acc: 0.0662 - val_loss: 10.8432 - val_acc: 0.0331\n",
      "Epoch 147/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8225 - acc: 0.0674 - val_loss: 10.8420 - val_acc: 0.0331\n",
      "Epoch 148/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8212 - acc: 0.0674 - val_loss: 10.8409 - val_acc: 0.0331\n",
      "Epoch 149/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8209 - acc: 0.0699 - val_loss: 10.8399 - val_acc: 0.0331\n",
      "Epoch 150/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8190 - acc: 0.0674 - val_loss: 10.8388 - val_acc: 0.0331\n",
      "Epoch 151/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8182 - acc: 0.0662 - val_loss: 10.8377 - val_acc: 0.0331\n",
      "Epoch 152/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8173 - acc: 0.0674 - val_loss: 10.8366 - val_acc: 0.0331\n",
      "Epoch 153/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8160 - acc: 0.0686 - val_loss: 10.8355 - val_acc: 0.0331\n",
      "Epoch 154/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8149 - acc: 0.0674 - val_loss: 10.8344 - val_acc: 0.0331\n",
      "Epoch 155/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8131 - acc: 0.0650 - val_loss: 10.8335 - val_acc: 0.0331\n",
      "Epoch 156/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8120 - acc: 0.0686 - val_loss: 10.8323 - val_acc: 0.0331\n",
      "Epoch 157/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8113 - acc: 0.0674 - val_loss: 10.8313 - val_acc: 0.0331\n",
      "Epoch 158/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8100 - acc: 0.0650 - val_loss: 10.8300 - val_acc: 0.0331\n",
      "Epoch 159/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8089 - acc: 0.0674 - val_loss: 10.8289 - val_acc: 0.0331\n",
      "Epoch 160/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8078 - acc: 0.0699 - val_loss: 10.8278 - val_acc: 0.0331\n",
      "Epoch 161/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8059 - acc: 0.0699 - val_loss: 10.8269 - val_acc: 0.0331\n",
      "Epoch 162/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8053 - acc: 0.0686 - val_loss: 10.8259 - val_acc: 0.0331\n",
      "Epoch 163/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8041 - acc: 0.0686 - val_loss: 10.8247 - val_acc: 0.0331\n",
      "Epoch 164/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8032 - acc: 0.0674 - val_loss: 10.8236 - val_acc: 0.0331\n",
      "Epoch 165/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8023 - acc: 0.0686 - val_loss: 10.8225 - val_acc: 0.0331\n",
      "Epoch 166/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.8009 - acc: 0.0686 - val_loss: 10.8214 - val_acc: 0.0331\n",
      "Epoch 167/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7999 - acc: 0.0674 - val_loss: 10.8203 - val_acc: 0.0331\n",
      "Epoch 168/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7984 - acc: 0.0674 - val_loss: 10.8192 - val_acc: 0.0331\n",
      "Epoch 169/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7976 - acc: 0.0686 - val_loss: 10.8181 - val_acc: 0.0331\n",
      "Epoch 170/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7969 - acc: 0.0699 - val_loss: 10.8170 - val_acc: 0.0331\n",
      "Epoch 171/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7949 - acc: 0.0686 - val_loss: 10.8160 - val_acc: 0.0331\n",
      "Epoch 172/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7946 - acc: 0.0662 - val_loss: 10.8148 - val_acc: 0.0331\n",
      "Epoch 173/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7930 - acc: 0.0674 - val_loss: 10.8136 - val_acc: 0.0331\n",
      "Epoch 174/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7919 - acc: 0.0674 - val_loss: 10.8125 - val_acc: 0.0331\n",
      "Epoch 175/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7908 - acc: 0.0674 - val_loss: 10.8114 - val_acc: 0.0331\n",
      "Epoch 176/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7896 - acc: 0.0674 - val_loss: 10.8103 - val_acc: 0.0331\n",
      "Epoch 177/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7885 - acc: 0.0699 - val_loss: 10.8091 - val_acc: 0.0331\n",
      "Epoch 178/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7879 - acc: 0.0662 - val_loss: 10.8081 - val_acc: 0.0331\n",
      "Epoch 179/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7862 - acc: 0.0686 - val_loss: 10.8070 - val_acc: 0.0331\n",
      "Epoch 180/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7853 - acc: 0.0674 - val_loss: 10.8058 - val_acc: 0.0331\n",
      "Epoch 181/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7835 - acc: 0.0686 - val_loss: 10.8048 - val_acc: 0.0331\n",
      "Epoch 182/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7833 - acc: 0.0686 - val_loss: 10.8037 - val_acc: 0.0331\n",
      "Epoch 183/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7815 - acc: 0.0674 - val_loss: 10.8026 - val_acc: 0.0331\n",
      "Epoch 184/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7802 - acc: 0.0686 - val_loss: 10.8014 - val_acc: 0.0331\n",
      "Epoch 185/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7791 - acc: 0.0686 - val_loss: 10.8005 - val_acc: 0.0331\n",
      "Epoch 186/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7782 - acc: 0.0674 - val_loss: 10.7994 - val_acc: 0.0331\n",
      "Epoch 187/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7765 - acc: 0.0699 - val_loss: 10.7983 - val_acc: 0.0331\n",
      "Epoch 188/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7760 - acc: 0.0699 - val_loss: 10.7972 - val_acc: 0.0331\n",
      "Epoch 189/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7742 - acc: 0.0674 - val_loss: 10.7960 - val_acc: 0.0331\n",
      "Epoch 190/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7737 - acc: 0.0674 - val_loss: 10.7948 - val_acc: 0.0331\n",
      "Epoch 191/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7720 - acc: 0.0686 - val_loss: 10.7935 - val_acc: 0.0331\n",
      "Epoch 192/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7710 - acc: 0.0674 - val_loss: 10.7924 - val_acc: 0.0331\n",
      "Epoch 193/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7702 - acc: 0.0674 - val_loss: 10.7912 - val_acc: 0.0331\n",
      "Epoch 194/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7690 - acc: 0.0650 - val_loss: 10.7900 - val_acc: 0.0331\n",
      "Epoch 195/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7675 - acc: 0.0686 - val_loss: 10.7887 - val_acc: 0.0331\n",
      "Epoch 196/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7664 - acc: 0.0662 - val_loss: 10.7876 - val_acc: 0.0331\n",
      "Epoch 197/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7655 - acc: 0.0711 - val_loss: 10.7864 - val_acc: 0.0331\n",
      "Epoch 198/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7652 - acc: 0.0674 - val_loss: 10.7852 - val_acc: 0.0331\n",
      "Epoch 199/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7635 - acc: 0.0650 - val_loss: 10.7842 - val_acc: 0.0331\n",
      "Epoch 200/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7624 - acc: 0.0674 - val_loss: 10.7829 - val_acc: 0.0331\n",
      "Epoch 201/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7611 - acc: 0.0674 - val_loss: 10.7818 - val_acc: 0.0331\n",
      "Epoch 202/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7593 - acc: 0.0662 - val_loss: 10.7805 - val_acc: 0.0331\n",
      "Epoch 203/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7589 - acc: 0.0686 - val_loss: 10.7793 - val_acc: 0.0331\n",
      "Epoch 204/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7567 - acc: 0.0686 - val_loss: 10.7781 - val_acc: 0.0331\n",
      "Epoch 205/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7566 - acc: 0.0662 - val_loss: 10.7769 - val_acc: 0.0331\n",
      "Epoch 206/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7546 - acc: 0.0711 - val_loss: 10.7756 - val_acc: 0.0331\n",
      "Epoch 207/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7534 - acc: 0.0674 - val_loss: 10.7745 - val_acc: 0.0331\n",
      "Epoch 208/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7524 - acc: 0.0686 - val_loss: 10.7734 - val_acc: 0.0331\n",
      "Epoch 209/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7513 - acc: 0.0711 - val_loss: 10.7722 - val_acc: 0.0331\n",
      "Epoch 210/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7503 - acc: 0.0686 - val_loss: 10.7710 - val_acc: 0.0331\n",
      "Epoch 211/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7489 - acc: 0.0686 - val_loss: 10.7697 - val_acc: 0.0331\n",
      "Epoch 212/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7478 - acc: 0.0674 - val_loss: 10.7685 - val_acc: 0.0331\n",
      "Epoch 213/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7464 - acc: 0.0674 - val_loss: 10.7673 - val_acc: 0.0331\n",
      "Epoch 214/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7445 - acc: 0.0735 - val_loss: 10.7660 - val_acc: 0.0331\n",
      "Epoch 215/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7441 - acc: 0.0674 - val_loss: 10.7648 - val_acc: 0.0331\n",
      "Epoch 216/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7426 - acc: 0.0674 - val_loss: 10.7636 - val_acc: 0.0331\n",
      "Epoch 217/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7408 - acc: 0.0686 - val_loss: 10.7625 - val_acc: 0.0331\n",
      "Epoch 218/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7399 - acc: 0.0711 - val_loss: 10.7612 - val_acc: 0.0331\n",
      "Epoch 219/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7387 - acc: 0.0748 - val_loss: 10.7599 - val_acc: 0.0331\n",
      "Epoch 220/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7385 - acc: 0.0760 - val_loss: 10.7586 - val_acc: 0.0331\n",
      "Epoch 221/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7369 - acc: 0.0711 - val_loss: 10.7574 - val_acc: 0.0331\n",
      "Epoch 222/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7351 - acc: 0.0699 - val_loss: 10.7559 - val_acc: 0.0331\n",
      "Epoch 223/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7338 - acc: 0.0711 - val_loss: 10.7545 - val_acc: 0.0331\n",
      "Epoch 224/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7318 - acc: 0.0723 - val_loss: 10.7530 - val_acc: 0.0331\n",
      "Epoch 225/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7313 - acc: 0.0748 - val_loss: 10.7515 - val_acc: 0.0404\n",
      "Epoch 226/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7297 - acc: 0.0760 - val_loss: 10.7501 - val_acc: 0.0404\n",
      "Epoch 227/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7290 - acc: 0.0723 - val_loss: 10.7486 - val_acc: 0.0368\n",
      "Epoch 228/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7271 - acc: 0.0735 - val_loss: 10.7471 - val_acc: 0.0404\n",
      "Epoch 229/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7258 - acc: 0.0821 - val_loss: 10.7457 - val_acc: 0.0404\n",
      "Epoch 230/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7238 - acc: 0.0821 - val_loss: 10.7440 - val_acc: 0.0404\n",
      "Epoch 231/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7230 - acc: 0.0833 - val_loss: 10.7422 - val_acc: 0.0441\n",
      "Epoch 232/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7208 - acc: 0.0870 - val_loss: 10.7405 - val_acc: 0.0404\n",
      "Epoch 233/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7204 - acc: 0.0772 - val_loss: 10.7388 - val_acc: 0.0404\n",
      "Epoch 234/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7174 - acc: 0.0931 - val_loss: 10.7368 - val_acc: 0.0515\n",
      "Epoch 235/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7158 - acc: 0.1005 - val_loss: 10.7350 - val_acc: 0.0441\n",
      "Epoch 236/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7128 - acc: 0.1005 - val_loss: 10.7328 - val_acc: 0.0515\n",
      "Epoch 237/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7110 - acc: 0.0931 - val_loss: 10.7306 - val_acc: 0.0551\n",
      "Epoch 238/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7100 - acc: 0.0956 - val_loss: 10.7282 - val_acc: 0.0551\n",
      "Epoch 239/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7082 - acc: 0.0919 - val_loss: 10.7258 - val_acc: 0.0588\n",
      "Epoch 240/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7064 - acc: 0.0968 - val_loss: 10.7232 - val_acc: 0.0625\n",
      "Epoch 241/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7037 - acc: 0.1078 - val_loss: 10.7203 - val_acc: 0.0588\n",
      "Epoch 242/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.7012 - acc: 0.1091 - val_loss: 10.7169 - val_acc: 0.0588\n",
      "Epoch 243/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.6968 - acc: 0.0931 - val_loss: 10.7127 - val_acc: 0.0735\n",
      "Epoch 244/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.6941 - acc: 0.1201 - val_loss: 10.7082 - val_acc: 0.0809\n",
      "Epoch 245/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.6921 - acc: 0.1213 - val_loss: 10.7027 - val_acc: 0.0809\n",
      "Epoch 246/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.6850 - acc: 0.1324 - val_loss: 10.6966 - val_acc: 0.0882\n",
      "Epoch 247/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.6790 - acc: 0.1348 - val_loss: 10.6883 - val_acc: 0.0772\n",
      "Epoch 248/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.6708 - acc: 0.1360 - val_loss: 10.6792 - val_acc: 0.0699\n",
      "Epoch 249/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.6627 - acc: 0.1005 - val_loss: 10.6661 - val_acc: 0.0735\n",
      "Epoch 250/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.6491 - acc: 0.1017 - val_loss: 10.6538 - val_acc: 0.0735\n",
      "Epoch 251/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.6329 - acc: 0.0797 - val_loss: 10.6321 - val_acc: 0.0772\n",
      "Epoch 252/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.6138 - acc: 0.0980 - val_loss: 10.6050 - val_acc: 0.0772\n",
      "Epoch 253/700\n",
      "816/816 [==============================] - 17s 21ms/step - loss: 10.5925 - acc: 0.0993 - val_loss: 10.5739 - val_acc: 0.0772\n",
      "Epoch 254/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.5657 - acc: 0.1042 - val_loss: 10.5576 - val_acc: 0.0846\n",
      "Epoch 255/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.5320 - acc: 0.1164 - val_loss: 10.5320 - val_acc: 0.0956\n",
      "Epoch 256/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.5750 - acc: 0.1213 - val_loss: 10.6251 - val_acc: 0.0772\n",
      "Epoch 257/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.5422 - acc: 0.1287 - val_loss: 10.4834 - val_acc: 0.0993\n",
      "Epoch 258/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.5023 - acc: 0.1422 - val_loss: 10.4796 - val_acc: 0.1507\n",
      "Epoch 259/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.5295 - acc: 0.1360 - val_loss: 10.4888 - val_acc: 0.1287\n",
      "Epoch 260/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.4848 - acc: 0.1556 - val_loss: 10.4277 - val_acc: 0.1691\n",
      "Epoch 261/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.4659 - acc: 0.1373 - val_loss: 10.4231 - val_acc: 0.1654\n",
      "Epoch 262/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.4605 - acc: 0.1544 - val_loss: 10.4243 - val_acc: 0.1654\n",
      "Epoch 263/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.4070 - acc: 0.1532 - val_loss: 10.4530 - val_acc: 0.1801\n",
      "Epoch 264/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.4608 - acc: 0.1618 - val_loss: 10.4626 - val_acc: 0.1176\n",
      "Epoch 265/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.3693 - acc: 0.1667 - val_loss: 10.5296 - val_acc: 0.1140\n",
      "Epoch 266/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.5324 - acc: 0.1348 - val_loss: 10.2883 - val_acc: 0.1949\n",
      "Epoch 267/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.3339 - acc: 0.1789 - val_loss: 10.4984 - val_acc: 0.1029\n",
      "Epoch 268/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.3654 - acc: 0.1618 - val_loss: 10.3231 - val_acc: 0.1654\n",
      "Epoch 269/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.3251 - acc: 0.2108 - val_loss: 10.2773 - val_acc: 0.1801\n",
      "Epoch 270/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.2879 - acc: 0.2047 - val_loss: 10.2332 - val_acc: 0.1912\n",
      "Epoch 271/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.3064 - acc: 0.1703 - val_loss: 10.3378 - val_acc: 0.2132\n",
      "Epoch 272/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.2242 - acc: 0.2022 - val_loss: 10.4758 - val_acc: 0.1985\n",
      "Epoch 273/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.3729 - acc: 0.1838 - val_loss: 10.2218 - val_acc: 0.2426\n",
      "Epoch 274/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.1759 - acc: 0.2292 - val_loss: 10.3179 - val_acc: 0.2206\n",
      "Epoch 275/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.2129 - acc: 0.2402 - val_loss: 10.3345 - val_acc: 0.1985\n",
      "Epoch 276/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.1931 - acc: 0.2083 - val_loss: 10.4187 - val_acc: 0.2206\n",
      "Epoch 277/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.2219 - acc: 0.2255 - val_loss: 10.3138 - val_acc: 0.2279\n",
      "Epoch 278/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.1256 - acc: 0.2549 - val_loss: 10.4656 - val_acc: 0.2169\n",
      "Epoch 279/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.1017 - acc: 0.2439 - val_loss: 10.1353 - val_acc: 0.2353\n",
      "Epoch 280/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.2448 - acc: 0.2083 - val_loss: 10.0623 - val_acc: 0.2353\n",
      "Epoch 281/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.0289 - acc: 0.2659 - val_loss: 10.2520 - val_acc: 0.1985\n",
      "Epoch 282/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.1542 - acc: 0.2365 - val_loss: 10.2908 - val_acc: 0.2390\n",
      "Epoch 283/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.0702 - acc: 0.2426 - val_loss: 10.0055 - val_acc: 0.2794\n",
      "Epoch 284/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.0095 - acc: 0.2598 - val_loss: 10.0689 - val_acc: 0.2684\n",
      "Epoch 285/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.1159 - acc: 0.2463 - val_loss: 10.0146 - val_acc: 0.2721\n",
      "Epoch 286/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.9743 - acc: 0.2819 - val_loss: 10.2486 - val_acc: 0.2463\n",
      "Epoch 287/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.9583 - acc: 0.2696 - val_loss: 10.0692 - val_acc: 0.2684\n",
      "Epoch 288/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.1024 - acc: 0.2696 - val_loss: 9.9823 - val_acc: 0.2610\n",
      "Epoch 289/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.8928 - acc: 0.2880 - val_loss: 9.9362 - val_acc: 0.2831\n",
      "Epoch 290/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 10.0118 - acc: 0.2684 - val_loss: 9.9519 - val_acc: 0.2941\n",
      "Epoch 291/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.9273 - acc: 0.3039 - val_loss: 10.0108 - val_acc: 0.2941\n",
      "Epoch 292/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.9871 - acc: 0.2757 - val_loss: 9.9225 - val_acc: 0.2721\n",
      "Epoch 293/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.8866 - acc: 0.2978 - val_loss: 10.2187 - val_acc: 0.2647\n",
      "Epoch 294/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.8612 - acc: 0.3199 - val_loss: 10.0287 - val_acc: 0.2647\n",
      "Epoch 295/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.9330 - acc: 0.3051 - val_loss: 9.9756 - val_acc: 0.2831\n",
      "Epoch 296/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.8436 - acc: 0.2978 - val_loss: 10.0463 - val_acc: 0.2721\n",
      "Epoch 297/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.8129 - acc: 0.3272 - val_loss: 10.0680 - val_acc: 0.2684\n",
      "Epoch 298/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.9528 - acc: 0.3162 - val_loss: 10.0545 - val_acc: 0.2463\n",
      "Epoch 299/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.9027 - acc: 0.2843 - val_loss: 9.8536 - val_acc: 0.2868\n",
      "Epoch 300/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.7722 - acc: 0.3419 - val_loss: 9.9007 - val_acc: 0.2868\n",
      "Epoch 301/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.7868 - acc: 0.3137 - val_loss: 9.9341 - val_acc: 0.2757\n",
      "Epoch 302/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.8189 - acc: 0.3407 - val_loss: 9.8133 - val_acc: 0.2904\n",
      "Epoch 303/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.7475 - acc: 0.3578 - val_loss: 10.0623 - val_acc: 0.3015\n",
      "Epoch 304/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.7296 - acc: 0.3505 - val_loss: 9.9475 - val_acc: 0.2610\n",
      "Epoch 305/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.7325 - acc: 0.3480 - val_loss: 9.8281 - val_acc: 0.3015\n",
      "Epoch 306/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.7642 - acc: 0.3456 - val_loss: 9.9919 - val_acc: 0.2647\n",
      "Epoch 307/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.7899 - acc: 0.3223 - val_loss: 9.9029 - val_acc: 0.2794\n",
      "Epoch 308/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.7160 - acc: 0.3284 - val_loss: 9.9092 - val_acc: 0.2904\n",
      "Epoch 309/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.6845 - acc: 0.3517 - val_loss: 9.9071 - val_acc: 0.2978\n",
      "Epoch 310/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.6890 - acc: 0.3444 - val_loss: 9.9591 - val_acc: 0.2941\n",
      "Epoch 311/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.7718 - acc: 0.3309 - val_loss: 9.9099 - val_acc: 0.2831\n",
      "Epoch 312/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.6761 - acc: 0.3627 - val_loss: 9.9582 - val_acc: 0.2794\n",
      "Epoch 313/700\n",
      "816/816 [==============================] - 17s 21ms/step - loss: 9.7485 - acc: 0.3333 - val_loss: 10.0929 - val_acc: 0.3162\n",
      "Epoch 314/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.6925 - acc: 0.3603 - val_loss: 9.7354 - val_acc: 0.3456\n",
      "Epoch 315/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.6384 - acc: 0.3615 - val_loss: 9.8620 - val_acc: 0.3015\n",
      "Epoch 316/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.6016 - acc: 0.3860 - val_loss: 9.8075 - val_acc: 0.3015\n",
      "Epoch 317/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.5997 - acc: 0.4044 - val_loss: 10.1176 - val_acc: 0.2904\n",
      "Epoch 318/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.6296 - acc: 0.3701 - val_loss: 9.9048 - val_acc: 0.3162\n",
      "Epoch 319/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.7065 - acc: 0.3664 - val_loss: 9.7608 - val_acc: 0.3272\n",
      "Epoch 320/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.5483 - acc: 0.3860 - val_loss: 9.8576 - val_acc: 0.3125\n",
      "Epoch 321/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.5166 - acc: 0.4081 - val_loss: 9.7211 - val_acc: 0.3382\n",
      "Epoch 322/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.5331 - acc: 0.3958 - val_loss: 9.7522 - val_acc: 0.3272\n",
      "Epoch 323/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.5827 - acc: 0.3922 - val_loss: 9.7225 - val_acc: 0.3419\n",
      "Epoch 324/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.5519 - acc: 0.3995 - val_loss: 9.9149 - val_acc: 0.3309\n",
      "Epoch 325/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.5249 - acc: 0.4105 - val_loss: 9.8875 - val_acc: 0.3456\n",
      "Epoch 326/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.5453 - acc: 0.4056 - val_loss: 9.7149 - val_acc: 0.3529\n",
      "Epoch 327/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.4910 - acc: 0.4081 - val_loss: 9.9405 - val_acc: 0.2978\n",
      "Epoch 328/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.5757 - acc: 0.4081 - val_loss: 9.8167 - val_acc: 0.3309\n",
      "Epoch 329/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.5688 - acc: 0.4020 - val_loss: 9.9753 - val_acc: 0.3456\n",
      "Epoch 330/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.4151 - acc: 0.4424 - val_loss: 10.0965 - val_acc: 0.3235\n",
      "Epoch 331/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.4664 - acc: 0.4277 - val_loss: 9.9005 - val_acc: 0.2904\n",
      "Epoch 332/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.4816 - acc: 0.4363 - val_loss: 9.7460 - val_acc: 0.3419\n",
      "Epoch 333/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.3920 - acc: 0.4522 - val_loss: 9.7200 - val_acc: 0.3493\n",
      "Epoch 334/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.4699 - acc: 0.4412 - val_loss: 9.8646 - val_acc: 0.3235\n",
      "Epoch 335/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.4777 - acc: 0.4130 - val_loss: 9.7760 - val_acc: 0.3309\n",
      "Epoch 336/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.3665 - acc: 0.4743 - val_loss: 9.8714 - val_acc: 0.3235\n",
      "Epoch 337/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.3680 - acc: 0.4449 - val_loss: 9.6701 - val_acc: 0.3713\n",
      "Epoch 338/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.3333 - acc: 0.4816 - val_loss: 9.7419 - val_acc: 0.3529\n",
      "Epoch 339/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.3185 - acc: 0.4645 - val_loss: 9.8353 - val_acc: 0.3162\n",
      "Epoch 340/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.3479 - acc: 0.4730 - val_loss: 9.7617 - val_acc: 0.3456\n",
      "Epoch 341/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.2784 - acc: 0.4779 - val_loss: 9.6025 - val_acc: 0.3676\n",
      "Epoch 342/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.2302 - acc: 0.5172 - val_loss: 10.6306 - val_acc: 0.2684\n",
      "Epoch 343/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.5898 - acc: 0.4179 - val_loss: 9.8578 - val_acc: 0.3640\n",
      "Epoch 344/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.2078 - acc: 0.5025 - val_loss: 9.7078 - val_acc: 0.3713\n",
      "Epoch 345/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.2307 - acc: 0.4865 - val_loss: 9.8091 - val_acc: 0.3713\n",
      "Epoch 346/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.1399 - acc: 0.5515 - val_loss: 9.9773 - val_acc: 0.3199\n",
      "Epoch 347/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.3165 - acc: 0.4926 - val_loss: 9.8935 - val_acc: 0.3640\n",
      "Epoch 348/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.1293 - acc: 0.5368 - val_loss: 9.7128 - val_acc: 0.3934\n",
      "Epoch 349/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.1901 - acc: 0.5086 - val_loss: 9.7795 - val_acc: 0.3382\n",
      "Epoch 350/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.1742 - acc: 0.5257 - val_loss: 9.9065 - val_acc: 0.3382\n",
      "Epoch 351/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.1758 - acc: 0.5184 - val_loss: 10.0622 - val_acc: 0.3456\n",
      "Epoch 352/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.1575 - acc: 0.5331 - val_loss: 10.1624 - val_acc: 0.3419\n",
      "Epoch 353/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.1687 - acc: 0.5012 - val_loss: 9.7498 - val_acc: 0.3676\n",
      "Epoch 354/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.1478 - acc: 0.5245 - val_loss: 9.8803 - val_acc: 0.3860\n",
      "Epoch 355/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.0336 - acc: 0.5674 - val_loss: 10.0290 - val_acc: 0.3713\n",
      "Epoch 356/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.9997 - acc: 0.5784 - val_loss: 9.7682 - val_acc: 0.3603\n",
      "Epoch 357/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.9959 - acc: 0.5760 - val_loss: 10.1078 - val_acc: 0.3750\n",
      "Epoch 358/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.0142 - acc: 0.5735 - val_loss: 10.1160 - val_acc: 0.3566\n",
      "Epoch 359/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.0038 - acc: 0.5588 - val_loss: 10.3245 - val_acc: 0.3346\n",
      "Epoch 360/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.1860 - acc: 0.5049 - val_loss: 9.8752 - val_acc: 0.3787\n",
      "Epoch 361/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.9026 - acc: 0.5944 - val_loss: 9.9810 - val_acc: 0.3713\n",
      "Epoch 362/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.9243 - acc: 0.5821 - val_loss: 10.0371 - val_acc: 0.3676\n",
      "Epoch 363/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.9180 - acc: 0.5993 - val_loss: 10.7470 - val_acc: 0.3493\n",
      "Epoch 364/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.9113 - acc: 0.6115 - val_loss: 9.9853 - val_acc: 0.4044\n",
      "Epoch 365/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.8732 - acc: 0.6275 - val_loss: 10.2919 - val_acc: 0.3787\n",
      "Epoch 366/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.8525 - acc: 0.6078 - val_loss: 9.7919 - val_acc: 0.4081\n",
      "Epoch 367/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.7921 - acc: 0.6422 - val_loss: 10.2386 - val_acc: 0.3787\n",
      "Epoch 368/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.8460 - acc: 0.6287 - val_loss: 10.0517 - val_acc: 0.3750\n",
      "Epoch 369/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.7843 - acc: 0.6275 - val_loss: 10.0392 - val_acc: 0.4044\n",
      "Epoch 370/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.7415 - acc: 0.6642 - val_loss: 10.1777 - val_acc: 0.3713\n",
      "Epoch 371/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.9003 - acc: 0.6176 - val_loss: 10.0419 - val_acc: 0.3750\n",
      "Epoch 372/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.8398 - acc: 0.6225 - val_loss: 10.1613 - val_acc: 0.3750\n",
      "Epoch 373/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.6227 - acc: 0.6985 - val_loss: 10.2949 - val_acc: 0.3860\n",
      "Epoch 374/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.7349 - acc: 0.6605 - val_loss: 10.2097 - val_acc: 0.4154\n",
      "Epoch 375/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.6301 - acc: 0.6912 - val_loss: 10.2512 - val_acc: 0.4007\n",
      "Epoch 376/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.6438 - acc: 0.6801 - val_loss: 10.5455 - val_acc: 0.4007\n",
      "Epoch 377/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.6661 - acc: 0.6740 - val_loss: 10.0498 - val_acc: 0.3860\n",
      "Epoch 378/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.7381 - acc: 0.6777 - val_loss: 9.9845 - val_acc: 0.3750\n",
      "Epoch 379/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.5240 - acc: 0.7304 - val_loss: 10.2090 - val_acc: 0.4228\n",
      "Epoch 380/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.4783 - acc: 0.7426 - val_loss: 10.7351 - val_acc: 0.4007\n",
      "Epoch 381/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.4541 - acc: 0.7488 - val_loss: 10.8173 - val_acc: 0.3603\n",
      "Epoch 382/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.6021 - acc: 0.6998 - val_loss: 10.6243 - val_acc: 0.3787\n",
      "Epoch 383/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.4866 - acc: 0.7402 - val_loss: 10.9332 - val_acc: 0.3860\n",
      "Epoch 384/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.6622 - acc: 0.6985 - val_loss: 10.3700 - val_acc: 0.3750\n",
      "Epoch 385/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.4378 - acc: 0.7561 - val_loss: 11.7476 - val_acc: 0.3272\n",
      "Epoch 386/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.6392 - acc: 0.7071 - val_loss: 10.4132 - val_acc: 0.3676\n",
      "Epoch 387/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.5016 - acc: 0.7426 - val_loss: 10.3363 - val_acc: 0.3934\n",
      "Epoch 388/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.3546 - acc: 0.7843 - val_loss: 10.7775 - val_acc: 0.4044\n",
      "Epoch 389/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.4378 - acc: 0.7561 - val_loss: 10.5339 - val_acc: 0.3603\n",
      "Epoch 390/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.5561 - acc: 0.7390 - val_loss: 10.5617 - val_acc: 0.3787\n",
      "Epoch 391/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.3854 - acc: 0.7733 - val_loss: 11.1763 - val_acc: 0.3272\n",
      "Epoch 392/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.5547 - acc: 0.7475 - val_loss: 10.5351 - val_acc: 0.3860\n",
      "Epoch 393/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.3327 - acc: 0.7966 - val_loss: 10.6320 - val_acc: 0.3713\n",
      "Epoch 394/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.2924 - acc: 0.8076 - val_loss: 10.8931 - val_acc: 0.3860\n",
      "Epoch 395/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.3592 - acc: 0.7708 - val_loss: 10.8745 - val_acc: 0.3971\n",
      "Epoch 396/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.3588 - acc: 0.7806 - val_loss: 11.0244 - val_acc: 0.3529\n",
      "Epoch 397/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.3591 - acc: 0.7953 - val_loss: 10.8315 - val_acc: 0.3713\n",
      "Epoch 398/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.2515 - acc: 0.8064 - val_loss: 10.8464 - val_acc: 0.3860\n",
      "Epoch 399/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.2481 - acc: 0.8051 - val_loss: 11.0318 - val_acc: 0.3824\n",
      "Epoch 400/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.1852 - acc: 0.8407 - val_loss: 11.3501 - val_acc: 0.4118\n",
      "Epoch 401/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.3295 - acc: 0.7978 - val_loss: 10.2286 - val_acc: 0.3529\n",
      "Epoch 402/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.2619 - acc: 0.8076 - val_loss: 11.6175 - val_acc: 0.3456\n",
      "Epoch 403/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.2579 - acc: 0.8174 - val_loss: 10.8862 - val_acc: 0.4154\n",
      "Epoch 404/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.1509 - acc: 0.8468 - val_loss: 10.9702 - val_acc: 0.3676\n",
      "Epoch 405/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.2509 - acc: 0.8150 - val_loss: 10.9688 - val_acc: 0.3860\n",
      "Epoch 406/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.2841 - acc: 0.8162 - val_loss: 10.9237 - val_acc: 0.3860\n",
      "Epoch 407/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.2423 - acc: 0.8358 - val_loss: 11.0975 - val_acc: 0.3824\n",
      "Epoch 408/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.1361 - acc: 0.8493 - val_loss: 10.7216 - val_acc: 0.4044\n",
      "Epoch 409/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.1736 - acc: 0.8444 - val_loss: 11.0414 - val_acc: 0.3860\n",
      "Epoch 410/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.1476 - acc: 0.8493 - val_loss: 11.2952 - val_acc: 0.4118\n",
      "Epoch 411/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.1551 - acc: 0.8542 - val_loss: 11.3199 - val_acc: 0.3824\n",
      "Epoch 412/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.1658 - acc: 0.8321 - val_loss: 11.0272 - val_acc: 0.3934\n",
      "Epoch 413/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.1093 - acc: 0.8664 - val_loss: 11.1968 - val_acc: 0.4007\n",
      "Epoch 414/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.1021 - acc: 0.8566 - val_loss: 11.6088 - val_acc: 0.4081\n",
      "Epoch 415/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.0455 - acc: 0.8873 - val_loss: 11.5710 - val_acc: 0.4228\n",
      "Epoch 416/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.0369 - acc: 0.8860 - val_loss: 11.7214 - val_acc: 0.4007\n",
      "Epoch 417/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.0844 - acc: 0.8689 - val_loss: 12.4450 - val_acc: 0.3640\n",
      "Epoch 418/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.6053 - acc: 0.7623 - val_loss: 10.9229 - val_acc: 0.3787\n",
      "Epoch 419/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.0307 - acc: 0.8958 - val_loss: 11.1098 - val_acc: 0.4228\n",
      "Epoch 420/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.0660 - acc: 0.8799 - val_loss: 11.5628 - val_acc: 0.3897\n",
      "Epoch 421/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9839 - acc: 0.8971 - val_loss: 11.5009 - val_acc: 0.4265\n",
      "Epoch 422/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.0218 - acc: 0.8873 - val_loss: 11.8104 - val_acc: 0.3456\n",
      "Epoch 423/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.2434 - acc: 0.8493 - val_loss: 11.3693 - val_acc: 0.4081\n",
      "Epoch 424/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9914 - acc: 0.9007 - val_loss: 11.6840 - val_acc: 0.4081\n",
      "Epoch 425/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9848 - acc: 0.9056 - val_loss: 11.7812 - val_acc: 0.3640\n",
      "Epoch 426/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.2108 - acc: 0.8603 - val_loss: 11.1194 - val_acc: 0.4338\n",
      "Epoch 427/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.1012 - acc: 0.8799 - val_loss: 10.9629 - val_acc: 0.3934\n",
      "Epoch 428/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9581 - acc: 0.9105 - val_loss: 11.6737 - val_acc: 0.3787\n",
      "Epoch 429/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9247 - acc: 0.9252 - val_loss: 11.9864 - val_acc: 0.3934\n",
      "Epoch 430/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9608 - acc: 0.9154 - val_loss: 12.0794 - val_acc: 0.3566\n",
      "Epoch 431/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.0732 - acc: 0.8873 - val_loss: 11.6301 - val_acc: 0.3566\n",
      "Epoch 432/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9636 - acc: 0.9044 - val_loss: 11.3415 - val_acc: 0.3676\n",
      "Epoch 433/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9910 - acc: 0.9118 - val_loss: 13.5297 - val_acc: 0.2831\n",
      "Epoch 434/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.8053 - acc: 0.7120 - val_loss: 10.9838 - val_acc: 0.3750\n",
      "Epoch 435/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9914 - acc: 0.9093 - val_loss: 11.7484 - val_acc: 0.3824\n",
      "Epoch 436/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9308 - acc: 0.9228 - val_loss: 11.9998 - val_acc: 0.3713\n",
      "Epoch 437/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9295 - acc: 0.9228 - val_loss: 11.7924 - val_acc: 0.3824\n",
      "Epoch 438/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9737 - acc: 0.9105 - val_loss: 11.6791 - val_acc: 0.3750\n",
      "Epoch 439/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9330 - acc: 0.9203 - val_loss: 11.9056 - val_acc: 0.3787\n",
      "Epoch 440/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9189 - acc: 0.9203 - val_loss: 12.8083 - val_acc: 0.3051\n",
      "Epoch 441/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.4399 - acc: 0.8027 - val_loss: 11.1830 - val_acc: 0.4044\n",
      "Epoch 442/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9389 - acc: 0.9203 - val_loss: 11.5281 - val_acc: 0.4007\n",
      "Epoch 443/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9229 - acc: 0.9179 - val_loss: 11.8428 - val_acc: 0.3787\n",
      "Epoch 444/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9026 - acc: 0.9314 - val_loss: 12.0189 - val_acc: 0.4154\n",
      "Epoch 445/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9302 - acc: 0.9216 - val_loss: 12.0821 - val_acc: 0.3971\n",
      "Epoch 446/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8852 - acc: 0.9252 - val_loss: 11.9130 - val_acc: 0.3934\n",
      "Epoch 447/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9094 - acc: 0.9301 - val_loss: 12.0652 - val_acc: 0.3897\n",
      "Epoch 448/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9587 - acc: 0.9167 - val_loss: 11.6941 - val_acc: 0.3860\n",
      "Epoch 449/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8724 - acc: 0.9289 - val_loss: 11.7442 - val_acc: 0.3971\n",
      "Epoch 450/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9272 - acc: 0.9118 - val_loss: 11.8652 - val_acc: 0.3787\n",
      "Epoch 451/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8703 - acc: 0.9338 - val_loss: 11.9892 - val_acc: 0.3934\n",
      "Epoch 452/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8753 - acc: 0.9363 - val_loss: 11.9462 - val_acc: 0.4044\n",
      "Epoch 453/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8703 - acc: 0.9387 - val_loss: 12.4983 - val_acc: 0.3640\n",
      "Epoch 454/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9761 - acc: 0.8983 - val_loss: 12.0847 - val_acc: 0.3713\n",
      "Epoch 455/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9332 - acc: 0.9105 - val_loss: 11.7087 - val_acc: 0.3971\n",
      "Epoch 456/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9115 - acc: 0.9277 - val_loss: 11.9236 - val_acc: 0.3750\n",
      "Epoch 457/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8140 - acc: 0.9571 - val_loss: 12.0401 - val_acc: 0.4044\n",
      "Epoch 458/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8388 - acc: 0.9412 - val_loss: 12.6110 - val_acc: 0.3971\n",
      "Epoch 459/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8680 - acc: 0.9228 - val_loss: 12.4529 - val_acc: 0.3713\n",
      "Epoch 460/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9052 - acc: 0.9240 - val_loss: 12.0317 - val_acc: 0.3860\n",
      "Epoch 461/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8483 - acc: 0.9449 - val_loss: 12.3230 - val_acc: 0.3897\n",
      "Epoch 462/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8203 - acc: 0.9473 - val_loss: 12.6223 - val_acc: 0.3676\n",
      "Epoch 463/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8630 - acc: 0.9338 - val_loss: 12.2939 - val_acc: 0.4081\n",
      "Epoch 464/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7956 - acc: 0.9645 - val_loss: 12.6881 - val_acc: 0.3971\n",
      "Epoch 465/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8707 - acc: 0.9167 - val_loss: 12.2304 - val_acc: 0.3971\n",
      "Epoch 466/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8187 - acc: 0.9412 - val_loss: 12.0445 - val_acc: 0.3934\n",
      "Epoch 467/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8445 - acc: 0.9400 - val_loss: 11.9462 - val_acc: 0.3934\n",
      "Epoch 468/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8707 - acc: 0.9301 - val_loss: 12.3231 - val_acc: 0.3897\n",
      "Epoch 469/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8526 - acc: 0.9265 - val_loss: 12.2572 - val_acc: 0.3934\n",
      "Epoch 470/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8904 - acc: 0.9301 - val_loss: 12.0844 - val_acc: 0.4265\n",
      "Epoch 471/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8088 - acc: 0.9510 - val_loss: 12.1301 - val_acc: 0.4154\n",
      "Epoch 472/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7990 - acc: 0.9522 - val_loss: 12.6010 - val_acc: 0.3971\n",
      "Epoch 473/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7931 - acc: 0.9461 - val_loss: 12.5432 - val_acc: 0.3787\n",
      "Epoch 474/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8083 - acc: 0.9473 - val_loss: 12.2733 - val_acc: 0.4081\n",
      "Epoch 475/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8052 - acc: 0.9461 - val_loss: 12.4551 - val_acc: 0.3787\n",
      "Epoch 476/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8364 - acc: 0.9424 - val_loss: 12.2750 - val_acc: 0.3860\n",
      "Epoch 477/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8956 - acc: 0.9142 - val_loss: 12.2404 - val_acc: 0.3897\n",
      "Epoch 478/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8327 - acc: 0.9436 - val_loss: 12.1143 - val_acc: 0.3971\n",
      "Epoch 479/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7745 - acc: 0.9608 - val_loss: 11.8121 - val_acc: 0.3934\n",
      "Epoch 480/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8187 - acc: 0.9510 - val_loss: 11.9988 - val_acc: 0.3971\n",
      "Epoch 481/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7716 - acc: 0.9571 - val_loss: 12.5198 - val_acc: 0.4044\n",
      "Epoch 482/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7962 - acc: 0.9485 - val_loss: 12.3181 - val_acc: 0.3934\n",
      "Epoch 483/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7902 - acc: 0.9461 - val_loss: 12.5077 - val_acc: 0.3566\n",
      "Epoch 484/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7947 - acc: 0.9510 - val_loss: 12.3293 - val_acc: 0.3897\n",
      "Epoch 485/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9901 - acc: 0.8946 - val_loss: 11.5904 - val_acc: 0.3787\n",
      "Epoch 486/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7597 - acc: 0.9559 - val_loss: 12.0665 - val_acc: 0.3750\n",
      "Epoch 487/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7784 - acc: 0.9522 - val_loss: 12.0465 - val_acc: 0.3897\n",
      "Epoch 488/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8021 - acc: 0.9424 - val_loss: 12.2146 - val_acc: 0.4044\n",
      "Epoch 489/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7265 - acc: 0.9706 - val_loss: 12.5706 - val_acc: 0.3860\n",
      "Epoch 490/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8461 - acc: 0.9326 - val_loss: 11.7818 - val_acc: 0.3824\n",
      "Epoch 491/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8092 - acc: 0.9449 - val_loss: 12.3810 - val_acc: 0.3750\n",
      "Epoch 492/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7518 - acc: 0.9645 - val_loss: 12.4511 - val_acc: 0.3971\n",
      "Epoch 493/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7995 - acc: 0.9510 - val_loss: 11.9089 - val_acc: 0.3897\n",
      "Epoch 494/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7692 - acc: 0.9547 - val_loss: 12.1658 - val_acc: 0.3971\n",
      "Epoch 495/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7476 - acc: 0.9596 - val_loss: 12.1732 - val_acc: 0.4007\n",
      "Epoch 496/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7770 - acc: 0.9510 - val_loss: 12.2542 - val_acc: 0.3971\n",
      "Epoch 497/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7367 - acc: 0.9657 - val_loss: 12.3653 - val_acc: 0.4007\n",
      "Epoch 498/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7158 - acc: 0.9694 - val_loss: 12.3757 - val_acc: 0.3897\n",
      "Epoch 499/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7165 - acc: 0.9669 - val_loss: 13.0312 - val_acc: 0.3787\n",
      "Epoch 500/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.7336 - acc: 0.9620 - val_loss: 12.6317 - val_acc: 0.3934\n",
      "Epoch 501/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.7414 - acc: 0.9681 - val_loss: 13.0661 - val_acc: 0.3897\n",
      "Epoch 502/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.7030 - acc: 0.9718 - val_loss: 12.9982 - val_acc: 0.3934\n",
      "Epoch 503/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.7373 - acc: 0.9608 - val_loss: 12.7319 - val_acc: 0.3824\n",
      "Epoch 504/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.7157 - acc: 0.9718 - val_loss: 12.8896 - val_acc: 0.3713\n",
      "Epoch 505/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.7104 - acc: 0.9718 - val_loss: 12.8566 - val_acc: 0.3750\n",
      "Epoch 506/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.7216 - acc: 0.9694 - val_loss: 13.2004 - val_acc: 0.3713\n",
      "Epoch 507/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7457 - acc: 0.9657 - val_loss: 12.8254 - val_acc: 0.3787\n",
      "Epoch 508/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7344 - acc: 0.9681 - val_loss: 12.7237 - val_acc: 0.3897\n",
      "Epoch 509/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7236 - acc: 0.9669 - val_loss: 13.2332 - val_acc: 0.3529\n",
      "Epoch 510/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8668 - acc: 0.9412 - val_loss: 12.3122 - val_acc: 0.3897\n",
      "Epoch 511/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7141 - acc: 0.9743 - val_loss: 12.2201 - val_acc: 0.3860\n",
      "Epoch 512/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7328 - acc: 0.9681 - val_loss: 12.4581 - val_acc: 0.3934\n",
      "Epoch 513/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7714 - acc: 0.9522 - val_loss: 12.2248 - val_acc: 0.3824\n",
      "Epoch 514/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7286 - acc: 0.9657 - val_loss: 12.0180 - val_acc: 0.3824\n",
      "Epoch 515/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7663 - acc: 0.9473 - val_loss: 12.2236 - val_acc: 0.3897\n",
      "Epoch 516/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7018 - acc: 0.9694 - val_loss: 12.3605 - val_acc: 0.4118\n",
      "Epoch 517/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6959 - acc: 0.9743 - val_loss: 12.4136 - val_acc: 0.3934\n",
      "Epoch 518/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.7010 - acc: 0.9743 - val_loss: 12.4405 - val_acc: 0.3897\n",
      "Epoch 519/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6952 - acc: 0.9730 - val_loss: 12.3392 - val_acc: 0.3860\n",
      "Epoch 520/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6898 - acc: 0.9730 - val_loss: 12.9079 - val_acc: 0.4081\n",
      "Epoch 521/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7672 - acc: 0.9510 - val_loss: 12.1295 - val_acc: 0.3713\n",
      "Epoch 522/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6937 - acc: 0.9694 - val_loss: 12.3528 - val_acc: 0.3897\n",
      "Epoch 523/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6808 - acc: 0.9755 - val_loss: 12.3889 - val_acc: 0.3934\n",
      "Epoch 524/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6688 - acc: 0.9792 - val_loss: 12.8978 - val_acc: 0.3934\n",
      "Epoch 525/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6764 - acc: 0.9743 - val_loss: 13.2493 - val_acc: 0.3897\n",
      "Epoch 526/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6854 - acc: 0.9730 - val_loss: 12.7260 - val_acc: 0.3971\n",
      "Epoch 527/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6780 - acc: 0.9743 - val_loss: 12.8688 - val_acc: 0.4154\n",
      "Epoch 528/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6788 - acc: 0.9730 - val_loss: 13.2359 - val_acc: 0.3787\n",
      "Epoch 529/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6803 - acc: 0.9718 - val_loss: 13.0570 - val_acc: 0.3750\n",
      "Epoch 530/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6918 - acc: 0.9694 - val_loss: 13.1606 - val_acc: 0.3897\n",
      "Epoch 531/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6857 - acc: 0.9755 - val_loss: 13.1973 - val_acc: 0.4007\n",
      "Epoch 532/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.9639 - acc: 0.8971 - val_loss: 11.2727 - val_acc: 0.3934\n",
      "Epoch 533/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6884 - acc: 0.9743 - val_loss: 12.0521 - val_acc: 0.3934\n",
      "Epoch 534/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6897 - acc: 0.9743 - val_loss: 12.2781 - val_acc: 0.3787\n",
      "Epoch 535/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6607 - acc: 0.9779 - val_loss: 12.3517 - val_acc: 0.3824\n",
      "Epoch 536/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6541 - acc: 0.9755 - val_loss: 12.6739 - val_acc: 0.3860\n",
      "Epoch 537/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6628 - acc: 0.9743 - val_loss: 12.6906 - val_acc: 0.3934\n",
      "Epoch 538/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6822 - acc: 0.9706 - val_loss: 12.5059 - val_acc: 0.3713\n",
      "Epoch 539/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6949 - acc: 0.9706 - val_loss: 12.6402 - val_acc: 0.3897\n",
      "Epoch 540/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6386 - acc: 0.9865 - val_loss: 12.7144 - val_acc: 0.3934\n",
      "Epoch 541/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6416 - acc: 0.9804 - val_loss: 12.8723 - val_acc: 0.3934\n",
      "Epoch 542/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6370 - acc: 0.9804 - val_loss: 12.9743 - val_acc: 0.4228\n",
      "Epoch 543/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6442 - acc: 0.9743 - val_loss: 12.9600 - val_acc: 0.3971\n",
      "Epoch 544/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6406 - acc: 0.9804 - val_loss: 13.6083 - val_acc: 0.3934\n",
      "Epoch 545/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6921 - acc: 0.9632 - val_loss: 12.6756 - val_acc: 0.3897\n",
      "Epoch 546/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6792 - acc: 0.9718 - val_loss: 12.5785 - val_acc: 0.4154\n",
      "Epoch 547/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6563 - acc: 0.9755 - val_loss: 12.7053 - val_acc: 0.3824\n",
      "Epoch 548/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6346 - acc: 0.9816 - val_loss: 12.8802 - val_acc: 0.3971\n",
      "Epoch 549/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6450 - acc: 0.9828 - val_loss: 12.6916 - val_acc: 0.3860\n",
      "Epoch 550/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6336 - acc: 0.9792 - val_loss: 12.5224 - val_acc: 0.4154\n",
      "Epoch 551/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7351 - acc: 0.9571 - val_loss: 12.4120 - val_acc: 0.3934\n",
      "Epoch 552/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6306 - acc: 0.9853 - val_loss: 12.7473 - val_acc: 0.4081\n",
      "Epoch 553/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6099 - acc: 0.9926 - val_loss: 12.8254 - val_acc: 0.3971\n",
      "Epoch 554/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6308 - acc: 0.9816 - val_loss: 13.0804 - val_acc: 0.3750\n",
      "Epoch 555/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6322 - acc: 0.9792 - val_loss: 13.1712 - val_acc: 0.3824\n",
      "Epoch 556/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6455 - acc: 0.9718 - val_loss: 12.6273 - val_acc: 0.3750\n",
      "Epoch 557/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6436 - acc: 0.9694 - val_loss: 13.0985 - val_acc: 0.4044\n",
      "Epoch 558/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6195 - acc: 0.9804 - val_loss: 12.7372 - val_acc: 0.4191\n",
      "Epoch 559/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6214 - acc: 0.9804 - val_loss: 12.7996 - val_acc: 0.4081\n",
      "Epoch 560/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6169 - acc: 0.9804 - val_loss: 13.2059 - val_acc: 0.4081\n",
      "Epoch 561/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6187 - acc: 0.9877 - val_loss: 13.1692 - val_acc: 0.4007\n",
      "Epoch 562/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6521 - acc: 0.9792 - val_loss: 13.2059 - val_acc: 0.3971\n",
      "Epoch 563/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6268 - acc: 0.9792 - val_loss: 13.2019 - val_acc: 0.3971\n",
      "Epoch 564/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6405 - acc: 0.9779 - val_loss: 13.0504 - val_acc: 0.4007\n",
      "Epoch 565/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6594 - acc: 0.9718 - val_loss: 12.9089 - val_acc: 0.3934\n",
      "Epoch 566/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.5981 - acc: 0.9914 - val_loss: 13.3049 - val_acc: 0.4154\n",
      "Epoch 567/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6294 - acc: 0.9792 - val_loss: 13.1904 - val_acc: 0.3860\n",
      "Epoch 568/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6592 - acc: 0.9743 - val_loss: 12.8583 - val_acc: 0.3860\n",
      "Epoch 569/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6095 - acc: 0.9841 - val_loss: 13.8317 - val_acc: 0.3640\n",
      "Epoch 570/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6111 - acc: 0.9841 - val_loss: 13.3802 - val_acc: 0.3971\n",
      "Epoch 571/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6004 - acc: 0.9816 - val_loss: 14.1093 - val_acc: 0.3824\n",
      "Epoch 572/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 12.3283 - acc: 0.4694 - val_loss: 10.0050 - val_acc: 0.2831\n",
      "Epoch 573/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 9.2148 - acc: 0.6115 - val_loss: 9.7827 - val_acc: 0.3309\n",
      "Epoch 574/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 8.1795 - acc: 0.8468 - val_loss: 11.2902 - val_acc: 0.3051\n",
      "Epoch 575/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8641 - acc: 0.9093 - val_loss: 11.8286 - val_acc: 0.3382\n",
      "Epoch 576/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8301 - acc: 0.9265 - val_loss: 11.9083 - val_acc: 0.3529\n",
      "Epoch 577/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7508 - acc: 0.9412 - val_loss: 12.5286 - val_acc: 0.3346\n",
      "Epoch 578/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7901 - acc: 0.9326 - val_loss: 12.5527 - val_acc: 0.3529\n",
      "Epoch 579/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7632 - acc: 0.9363 - val_loss: 13.6368 - val_acc: 0.3235\n",
      "Epoch 580/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7877 - acc: 0.9252 - val_loss: 12.5621 - val_acc: 0.3640\n",
      "Epoch 581/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7336 - acc: 0.9375 - val_loss: 12.9763 - val_acc: 0.3566\n",
      "Epoch 582/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7566 - acc: 0.9375 - val_loss: 12.5311 - val_acc: 0.3640\n",
      "Epoch 583/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7293 - acc: 0.9436 - val_loss: 12.8641 - val_acc: 0.3566\n",
      "Epoch 584/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7003 - acc: 0.9534 - val_loss: 13.0834 - val_acc: 0.3787\n",
      "Epoch 585/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7526 - acc: 0.9400 - val_loss: 13.1232 - val_acc: 0.3419\n",
      "Epoch 586/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7083 - acc: 0.9449 - val_loss: 13.1199 - val_acc: 0.3713\n",
      "Epoch 587/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.7340 - acc: 0.9424 - val_loss: 13.2981 - val_acc: 0.3676\n",
      "Epoch 588/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7362 - acc: 0.9412 - val_loss: 12.8055 - val_acc: 0.3676\n",
      "Epoch 589/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6873 - acc: 0.9559 - val_loss: 13.2423 - val_acc: 0.3824\n",
      "Epoch 590/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.7265 - acc: 0.9436 - val_loss: 12.3287 - val_acc: 0.3787\n",
      "Epoch 591/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7313 - acc: 0.9498 - val_loss: 12.6058 - val_acc: 0.3971\n",
      "Epoch 592/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6800 - acc: 0.9620 - val_loss: 12.8790 - val_acc: 0.3676\n",
      "Epoch 593/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6783 - acc: 0.9620 - val_loss: 13.0075 - val_acc: 0.3640\n",
      "Epoch 594/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7266 - acc: 0.9400 - val_loss: 12.9046 - val_acc: 0.3640\n",
      "Epoch 595/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6547 - acc: 0.9645 - val_loss: 13.5645 - val_acc: 0.3382\n",
      "Epoch 596/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.8133 - acc: 0.9498 - val_loss: 12.5633 - val_acc: 0.3713\n",
      "Epoch 597/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6483 - acc: 0.9730 - val_loss: 13.2370 - val_acc: 0.3603\n",
      "Epoch 598/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6604 - acc: 0.9645 - val_loss: 13.0706 - val_acc: 0.3713\n",
      "Epoch 599/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6507 - acc: 0.9571 - val_loss: 13.2595 - val_acc: 0.3456\n",
      "Epoch 600/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6753 - acc: 0.9620 - val_loss: 13.3247 - val_acc: 0.3529\n",
      "Epoch 601/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6270 - acc: 0.9718 - val_loss: 13.4871 - val_acc: 0.3603\n",
      "Epoch 602/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6919 - acc: 0.9583 - val_loss: 13.2030 - val_acc: 0.3676\n",
      "Epoch 603/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6470 - acc: 0.9706 - val_loss: 13.4254 - val_acc: 0.3713\n",
      "Epoch 604/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6492 - acc: 0.9645 - val_loss: 13.2755 - val_acc: 0.3566\n",
      "Epoch 605/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6251 - acc: 0.9706 - val_loss: 13.3863 - val_acc: 0.3493\n",
      "Epoch 606/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6424 - acc: 0.9632 - val_loss: 13.1201 - val_acc: 0.3787\n",
      "Epoch 607/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6133 - acc: 0.9816 - val_loss: 13.2835 - val_acc: 0.3860\n",
      "Epoch 608/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6641 - acc: 0.9706 - val_loss: 13.4296 - val_acc: 0.3750\n",
      "Epoch 609/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6599 - acc: 0.9669 - val_loss: 13.1899 - val_acc: 0.3640\n",
      "Epoch 610/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6450 - acc: 0.9743 - val_loss: 13.0850 - val_acc: 0.4007\n",
      "Epoch 611/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6258 - acc: 0.9730 - val_loss: 12.9747 - val_acc: 0.3860\n",
      "Epoch 612/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6228 - acc: 0.9694 - val_loss: 13.0865 - val_acc: 0.3750\n",
      "Epoch 613/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6222 - acc: 0.9730 - val_loss: 13.1384 - val_acc: 0.3640\n",
      "Epoch 614/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6223 - acc: 0.9694 - val_loss: 13.0805 - val_acc: 0.3787\n",
      "Epoch 615/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.7381 - acc: 0.9571 - val_loss: 12.4303 - val_acc: 0.3750\n",
      "Epoch 616/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6048 - acc: 0.9730 - val_loss: 12.9886 - val_acc: 0.3713\n",
      "Epoch 617/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5912 - acc: 0.9792 - val_loss: 13.0497 - val_acc: 0.3676\n",
      "Epoch 618/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5921 - acc: 0.9730 - val_loss: 13.4967 - val_acc: 0.3860\n",
      "Epoch 619/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6503 - acc: 0.9645 - val_loss: 12.8534 - val_acc: 0.3419\n",
      "Epoch 620/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6194 - acc: 0.9718 - val_loss: 13.0297 - val_acc: 0.3824\n",
      "Epoch 621/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5801 - acc: 0.9767 - val_loss: 13.1643 - val_acc: 0.3787\n",
      "Epoch 622/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5733 - acc: 0.9828 - val_loss: 13.1983 - val_acc: 0.3971\n",
      "Epoch 623/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5592 - acc: 0.9841 - val_loss: 13.4801 - val_acc: 0.3897\n",
      "Epoch 624/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5535 - acc: 0.9841 - val_loss: 13.6102 - val_acc: 0.4007\n",
      "Epoch 625/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5789 - acc: 0.9816 - val_loss: 13.6239 - val_acc: 0.3934\n",
      "Epoch 626/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6060 - acc: 0.9743 - val_loss: 13.5617 - val_acc: 0.3860\n",
      "Epoch 627/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6052 - acc: 0.9730 - val_loss: 13.7411 - val_acc: 0.3529\n",
      "Epoch 628/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6199 - acc: 0.9755 - val_loss: 13.2954 - val_acc: 0.3787\n",
      "Epoch 629/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6045 - acc: 0.9779 - val_loss: 13.3130 - val_acc: 0.3603\n",
      "Epoch 630/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5541 - acc: 0.9841 - val_loss: 13.3362 - val_acc: 0.3603\n",
      "Epoch 631/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5668 - acc: 0.9828 - val_loss: 13.4562 - val_acc: 0.3640\n",
      "Epoch 632/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5647 - acc: 0.9792 - val_loss: 13.4760 - val_acc: 0.3566\n",
      "Epoch 633/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5882 - acc: 0.9694 - val_loss: 13.3463 - val_acc: 0.3824\n",
      "Epoch 634/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5656 - acc: 0.9792 - val_loss: 13.2671 - val_acc: 0.3640\n",
      "Epoch 635/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5696 - acc: 0.9767 - val_loss: 13.3733 - val_acc: 0.3640\n",
      "Epoch 636/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5880 - acc: 0.9730 - val_loss: 13.3438 - val_acc: 0.3566\n",
      "Epoch 637/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.5783 - acc: 0.9743 - val_loss: 13.4130 - val_acc: 0.3529\n",
      "Epoch 638/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6429 - acc: 0.9620 - val_loss: 12.5554 - val_acc: 0.3750\n",
      "Epoch 639/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5635 - acc: 0.9779 - val_loss: 12.8410 - val_acc: 0.3676\n",
      "Epoch 640/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5477 - acc: 0.9779 - val_loss: 13.0183 - val_acc: 0.3713\n",
      "Epoch 641/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5467 - acc: 0.9792 - val_loss: 13.1800 - val_acc: 0.3860\n",
      "Epoch 642/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5494 - acc: 0.9792 - val_loss: 13.0840 - val_acc: 0.3897\n",
      "Epoch 643/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5467 - acc: 0.9755 - val_loss: 13.3079 - val_acc: 0.3713\n",
      "Epoch 644/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5316 - acc: 0.9816 - val_loss: 13.3505 - val_acc: 0.3824\n",
      "Epoch 645/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5399 - acc: 0.9779 - val_loss: 13.3488 - val_acc: 0.3934\n",
      "Epoch 646/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5637 - acc: 0.9743 - val_loss: 13.1812 - val_acc: 0.3897\n",
      "Epoch 647/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5384 - acc: 0.9816 - val_loss: 13.5230 - val_acc: 0.4007\n",
      "Epoch 648/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5725 - acc: 0.9755 - val_loss: 13.4860 - val_acc: 0.3640\n",
      "Epoch 649/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5572 - acc: 0.9755 - val_loss: 13.2603 - val_acc: 0.3824\n",
      "Epoch 650/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5406 - acc: 0.9779 - val_loss: 13.2840 - val_acc: 0.3934\n",
      "Epoch 651/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5259 - acc: 0.9853 - val_loss: 13.4139 - val_acc: 0.3713\n",
      "Epoch 652/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5483 - acc: 0.9755 - val_loss: 13.2483 - val_acc: 0.3713\n",
      "Epoch 653/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5187 - acc: 0.9853 - val_loss: 13.4608 - val_acc: 0.3676\n",
      "Epoch 654/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5429 - acc: 0.9792 - val_loss: 13.4457 - val_acc: 0.3897\n",
      "Epoch 655/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5211 - acc: 0.9816 - val_loss: 13.5617 - val_acc: 0.3897\n",
      "Epoch 656/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5544 - acc: 0.9743 - val_loss: 13.3272 - val_acc: 0.3971\n",
      "Epoch 657/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5653 - acc: 0.9755 - val_loss: 13.6385 - val_acc: 0.3787\n",
      "Epoch 658/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.5606 - acc: 0.9694 - val_loss: 13.1116 - val_acc: 0.3787\n",
      "Epoch 659/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5376 - acc: 0.9755 - val_loss: 13.1693 - val_acc: 0.3824\n",
      "Epoch 660/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5268 - acc: 0.9792 - val_loss: 13.3279 - val_acc: 0.3824\n",
      "Epoch 661/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5540 - acc: 0.9694 - val_loss: 13.0912 - val_acc: 0.3824\n",
      "Epoch 662/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.6035 - acc: 0.9694 - val_loss: 12.7155 - val_acc: 0.3897\n",
      "Epoch 663/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5309 - acc: 0.9779 - val_loss: 12.8024 - val_acc: 0.4118\n",
      "Epoch 664/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5088 - acc: 0.9865 - val_loss: 13.1061 - val_acc: 0.3860\n",
      "Epoch 665/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5337 - acc: 0.9816 - val_loss: 13.1028 - val_acc: 0.3897\n",
      "Epoch 666/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5158 - acc: 0.9841 - val_loss: 13.2061 - val_acc: 0.3897\n",
      "Epoch 667/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5242 - acc: 0.9816 - val_loss: 13.0906 - val_acc: 0.3824\n",
      "Epoch 668/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5038 - acc: 0.9828 - val_loss: 13.2684 - val_acc: 0.3713\n",
      "Epoch 669/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5093 - acc: 0.9828 - val_loss: 13.3119 - val_acc: 0.3860\n",
      "Epoch 670/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5084 - acc: 0.9779 - val_loss: 13.3599 - val_acc: 0.3860\n",
      "Epoch 671/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4986 - acc: 0.9865 - val_loss: 13.5215 - val_acc: 0.3860\n",
      "Epoch 672/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5160 - acc: 0.9792 - val_loss: 13.4014 - val_acc: 0.3860\n",
      "Epoch 673/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4999 - acc: 0.9853 - val_loss: 13.4440 - val_acc: 0.3897\n",
      "Epoch 674/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5111 - acc: 0.9779 - val_loss: 13.5303 - val_acc: 0.3897\n",
      "Epoch 675/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4914 - acc: 0.9890 - val_loss: 13.5948 - val_acc: 0.3971\n",
      "Epoch 676/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4962 - acc: 0.9853 - val_loss: 13.8079 - val_acc: 0.3934\n",
      "Epoch 677/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5145 - acc: 0.9804 - val_loss: 13.4882 - val_acc: 0.4044\n",
      "Epoch 678/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5258 - acc: 0.9792 - val_loss: 13.0638 - val_acc: 0.3713\n",
      "Epoch 679/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4974 - acc: 0.9828 - val_loss: 13.2884 - val_acc: 0.3750\n",
      "Epoch 680/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4887 - acc: 0.9841 - val_loss: 13.4235 - val_acc: 0.3971\n",
      "Epoch 681/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5070 - acc: 0.9755 - val_loss: 13.3616 - val_acc: 0.3897\n",
      "Epoch 682/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4850 - acc: 0.9853 - val_loss: 13.5803 - val_acc: 0.3787\n",
      "Epoch 683/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4972 - acc: 0.9828 - val_loss: 13.7012 - val_acc: 0.3897\n",
      "Epoch 684/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5109 - acc: 0.9804 - val_loss: 13.4180 - val_acc: 0.3934\n",
      "Epoch 685/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4823 - acc: 0.9865 - val_loss: 13.6521 - val_acc: 0.4081\n",
      "Epoch 686/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4997 - acc: 0.9841 - val_loss: 13.7707 - val_acc: 0.3493\n",
      "Epoch 687/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4923 - acc: 0.9865 - val_loss: 13.5361 - val_acc: 0.3713\n",
      "Epoch 688/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.5252 - acc: 0.9743 - val_loss: 13.4179 - val_acc: 0.3824\n",
      "Epoch 689/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.6510 - acc: 0.9559 - val_loss: 12.8459 - val_acc: 0.3787\n",
      "Epoch 690/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4940 - acc: 0.9853 - val_loss: 12.8441 - val_acc: 0.4081\n",
      "Epoch 691/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4911 - acc: 0.9841 - val_loss: 13.2380 - val_acc: 0.3603\n",
      "Epoch 692/700\n",
      "816/816 [==============================] - 16s 20ms/step - loss: 7.5204 - acc: 0.9804 - val_loss: 13.0073 - val_acc: 0.3566\n",
      "Epoch 693/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4740 - acc: 0.9865 - val_loss: 13.2567 - val_acc: 0.3971\n",
      "Epoch 694/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4779 - acc: 0.9853 - val_loss: 13.3808 - val_acc: 0.3897\n",
      "Epoch 695/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4813 - acc: 0.9828 - val_loss: 13.5210 - val_acc: 0.3713\n",
      "Epoch 696/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4825 - acc: 0.9841 - val_loss: 13.0198 - val_acc: 0.3971\n",
      "Epoch 697/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4851 - acc: 0.9828 - val_loss: 13.0041 - val_acc: 0.3934\n",
      "Epoch 698/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4695 - acc: 0.9865 - val_loss: 13.0377 - val_acc: 0.4044\n",
      "Epoch 699/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4809 - acc: 0.9841 - val_loss: 13.0503 - val_acc: 0.3971\n",
      "Epoch 700/700\n",
      "816/816 [==============================] - 16s 19ms/step - loss: 7.4701 - acc: 0.9890 - val_loss: 13.0794 - val_acc: 0.4007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f649c5940>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGD(lr=0.1, momentum=0.9, decay=0.0005)\n",
    "vgg19.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "vgg19.fit(X_train, Y_train, batch_size=128, epochs=700, validation_split=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "gwt1c2AO_0mx",
    "outputId": "2116917e-0a9c-4217-e578-8c9f31499dd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 2s 6ms/step\n",
      "Loss on test set:  13.60617458119112\n",
      "Accuracy on test set:  0.38235294117647056\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = vgg19.evaluate(X_test, Y_test, batch_size=128)\n",
    "print(\"Loss on test set: \", loss_and_metrics[0])\n",
    "print(\"Accuracy on test set: \", loss_and_metrics[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VGG19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
