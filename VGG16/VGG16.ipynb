{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "7Tf7zVH68Fzy",
    "outputId": "fe870841-5b94-4352-addf-1a6f7eca4323"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0811 16:34:02.231762 140166807578496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "W0811 16:34:02.233152 140166807578496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "W0811 16:34:02.244199 140166807578496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W0811 16:34:02.253982 140166807578496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W0811 16:34:02.268418 140166807578496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "W0811 16:34:02.269329 140166807578496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Activation, MaxPooling2D, Dropout, Flatten, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "import tflearn.datasets.oxflower17 as oxflower17\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbKGz8jG8F0A"
   },
   "outputs": [],
   "source": [
    "def vgg16_model(img_shape=(224, 224, 3), classes = 1000):\n",
    "    vgg16 = Sequential()\n",
    "\n",
    "    # Convolutional Layer 1\n",
    "    # Padding is same to keep the the spatial resolution same. (For 3x3 filter, padding = 1)\n",
    "    vgg16.add(Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\", input_shape=img_shape, kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 2\n",
    "    vgg16.add(Conv2D(filters=64, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "    \n",
    "    # Maxpooling Layer 1\n",
    "    vgg16.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Convolutional Layer 3\n",
    "    vgg16.add(Conv2D(filters=128, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 4\n",
    "    vgg16.add(Conv2D(filters=128, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "    \n",
    "    # Maxpooling Layer 2\n",
    "    vgg16.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Convolutional Layer 5\n",
    "    vgg16.add(Conv2D(filters=256, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 6\n",
    "    vgg16.add(Conv2D(filters=256, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 7\n",
    "    vgg16.add(Conv2D(filters=256, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "    \n",
    "    # Maxpooling Layer 3\n",
    "    vgg16.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Convolutional Layer 8\n",
    "    vgg16.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 9\n",
    "    vgg16.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 10\n",
    "    vgg16.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "    \n",
    "    # Maxpooling Layer 4\n",
    "    vgg16.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Convolutional Layer 11\n",
    "    vgg16.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 12\n",
    "    vgg16.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "    \n",
    "    # Convolutional Layer 13\n",
    "    vgg16.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "    \n",
    "    # Maxpooling Layer 5\n",
    "    vgg16.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "    # Dense Layer 1\n",
    "    vgg16.add(Flatten())\n",
    "    vgg16.add(Dropout(0.5))\n",
    "    vgg16.add(Dense(4096, kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "\n",
    "    # Dense Layer 2\n",
    "    vgg16.add(Dropout(0.5))\n",
    "    vgg16.add(Dense(4096, kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('relu'))\n",
    "\n",
    "    # Dense Layer 3\n",
    "    vgg16.add(Dense(classes, kernel_regularizer=l2(0.0005)))\n",
    "    vgg16.add(Activation('softmax'))\n",
    "    \n",
    "    return vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "Nk3W4JDN8F0G",
    "outputId": "e5ff3c78-7e0a-45b4-eb15-8f31c639fe90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0.0%        0 / 60270631\r",
      "  0.0%     8192 / 60270631"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Oxford 17 category Flower Dataset, Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% 60276736 / 60270631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Succesfully downloaded', '17flowers.tgz', 60270631, 'bytes.')\n",
      "File Extracted\n",
      "Starting to parse images...\n",
      "Parsing Done!\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "X, Y = oxflower17.load_data(one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "RzCpZm9m8F0S",
    "outputId": "28492a20-c973-4f32-9f61-35d327dbeef1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X's shape:  (1360, 224, 224, 3)\n",
      "Y's shape:  (1360, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"X's shape: \", X.shape)\n",
    "print(\"Y's shape: \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "2IGggOqB8F0Z",
    "outputId": "2d265103-c5e2-4caf-9279-6675ab6fb608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train's shape:  (1088, 224, 224, 3)\n",
      "Y_train's shape:  (1088, 17)\n",
      "X_test's shape:  (272, 224, 224, 3)\n",
      "Y_test's shape:  (272, 17)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "print(\"X_train's shape: \", X_train.shape)\n",
    "print(\"Y_train's shape: \", Y_train.shape)\n",
    "print(\"X_test's shape: \", X_test.shape)\n",
    "print(\"Y_test's shape: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "i4x7yYon8F0e",
    "outputId": "3892b3f7-99dc-4b7d-ea9e-6dc3fe4117c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0811 16:34:24.488744 140166807578496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0811 16:34:24.542089 140166807578496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0811 16:34:24.834921 140166807578496 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 224, 224, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 224, 224, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 112, 112, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 112, 112, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 17)                69649     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 17)                0         \n",
      "=================================================================\n",
      "Total params: 134,330,193\n",
      "Trainable params: 134,330,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16 = vgg16_model((224,224,3), 17)\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Us3NM_t78F0j",
    "outputId": "aeb410a1-98fc-47c9-f12f-638b82044a30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0811 16:34:24.936762 140166807578496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0811 16:34:25.272679 140166807578496 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 816 samples, validate on 272 samples\n",
      "Epoch 1/400\n",
      "816/816 [==============================] - 57s 70ms/step - loss: 10.3519 - acc: 0.0613 - val_loss: 10.3513 - val_acc: 0.0699\n",
      "Epoch 2/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.3506 - acc: 0.0662 - val_loss: 10.3504 - val_acc: 0.0735\n",
      "Epoch 3/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.3495 - acc: 0.0551 - val_loss: 10.3495 - val_acc: 0.0588\n",
      "Epoch 4/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.3485 - acc: 0.0650 - val_loss: 10.3487 - val_acc: 0.0588\n",
      "Epoch 5/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.3472 - acc: 0.0637 - val_loss: 10.3479 - val_acc: 0.0588\n",
      "Epoch 6/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.3459 - acc: 0.0723 - val_loss: 10.3470 - val_acc: 0.0588\n",
      "Epoch 7/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.3448 - acc: 0.0674 - val_loss: 10.3462 - val_acc: 0.0625\n",
      "Epoch 8/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.3435 - acc: 0.0686 - val_loss: 10.3453 - val_acc: 0.0699\n",
      "Epoch 9/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3424 - acc: 0.0821 - val_loss: 10.3445 - val_acc: 0.0662\n",
      "Epoch 10/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.3413 - acc: 0.0686 - val_loss: 10.3437 - val_acc: 0.0772\n",
      "Epoch 11/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.3401 - acc: 0.0662 - val_loss: 10.3428 - val_acc: 0.0735\n",
      "Epoch 12/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.3391 - acc: 0.0662 - val_loss: 10.3419 - val_acc: 0.0735\n",
      "Epoch 13/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.3378 - acc: 0.0821 - val_loss: 10.3410 - val_acc: 0.0882\n",
      "Epoch 14/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.3366 - acc: 0.0833 - val_loss: 10.3402 - val_acc: 0.0735\n",
      "Epoch 15/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.3355 - acc: 0.0699 - val_loss: 10.3393 - val_acc: 0.0882\n",
      "Epoch 16/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.3343 - acc: 0.0833 - val_loss: 10.3385 - val_acc: 0.0772\n",
      "Epoch 17/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.3331 - acc: 0.0772 - val_loss: 10.3376 - val_acc: 0.0735\n",
      "Epoch 18/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3320 - acc: 0.0760 - val_loss: 10.3369 - val_acc: 0.0809\n",
      "Epoch 19/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.3308 - acc: 0.0821 - val_loss: 10.3360 - val_acc: 0.0882\n",
      "Epoch 20/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3297 - acc: 0.0711 - val_loss: 10.3352 - val_acc: 0.0662\n",
      "Epoch 21/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3287 - acc: 0.0711 - val_loss: 10.3344 - val_acc: 0.0404\n",
      "Epoch 22/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.3273 - acc: 0.0784 - val_loss: 10.3335 - val_acc: 0.0404\n",
      "Epoch 23/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3263 - acc: 0.0662 - val_loss: 10.3325 - val_acc: 0.0699\n",
      "Epoch 24/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3253 - acc: 0.0699 - val_loss: 10.3317 - val_acc: 0.0404\n",
      "Epoch 25/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3241 - acc: 0.0809 - val_loss: 10.3309 - val_acc: 0.0404\n",
      "Epoch 26/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3230 - acc: 0.0833 - val_loss: 10.3301 - val_acc: 0.0404\n",
      "Epoch 27/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.3218 - acc: 0.0833 - val_loss: 10.3292 - val_acc: 0.0404\n",
      "Epoch 28/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3208 - acc: 0.0784 - val_loss: 10.3284 - val_acc: 0.0404\n",
      "Epoch 29/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.3194 - acc: 0.0699 - val_loss: 10.3275 - val_acc: 0.0404\n",
      "Epoch 30/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3186 - acc: 0.0809 - val_loss: 10.3266 - val_acc: 0.0404\n",
      "Epoch 31/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3171 - acc: 0.0699 - val_loss: 10.3258 - val_acc: 0.0404\n",
      "Epoch 32/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3158 - acc: 0.0833 - val_loss: 10.3249 - val_acc: 0.0404\n",
      "Epoch 33/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.3151 - acc: 0.0870 - val_loss: 10.3240 - val_acc: 0.0404\n",
      "Epoch 34/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.3139 - acc: 0.0760 - val_loss: 10.3231 - val_acc: 0.0404\n",
      "Epoch 35/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.3127 - acc: 0.0760 - val_loss: 10.3222 - val_acc: 0.0404\n",
      "Epoch 36/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3116 - acc: 0.0748 - val_loss: 10.3214 - val_acc: 0.0404\n",
      "Epoch 37/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3104 - acc: 0.0735 - val_loss: 10.3206 - val_acc: 0.0404\n",
      "Epoch 38/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3095 - acc: 0.0760 - val_loss: 10.3198 - val_acc: 0.0404\n",
      "Epoch 39/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.3082 - acc: 0.0821 - val_loss: 10.3189 - val_acc: 0.0404\n",
      "Epoch 40/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3072 - acc: 0.0809 - val_loss: 10.3180 - val_acc: 0.0404\n",
      "Epoch 41/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3061 - acc: 0.0748 - val_loss: 10.3172 - val_acc: 0.0404\n",
      "Epoch 42/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.3048 - acc: 0.0846 - val_loss: 10.3164 - val_acc: 0.0404\n",
      "Epoch 43/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3038 - acc: 0.0858 - val_loss: 10.3155 - val_acc: 0.0404\n",
      "Epoch 44/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3022 - acc: 0.0748 - val_loss: 10.3146 - val_acc: 0.0404\n",
      "Epoch 45/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.3013 - acc: 0.0760 - val_loss: 10.3138 - val_acc: 0.0404\n",
      "Epoch 46/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.3004 - acc: 0.0797 - val_loss: 10.3129 - val_acc: 0.0404\n",
      "Epoch 47/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2990 - acc: 0.0784 - val_loss: 10.3120 - val_acc: 0.0404\n",
      "Epoch 48/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2985 - acc: 0.0650 - val_loss: 10.3110 - val_acc: 0.0404\n",
      "Epoch 49/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2971 - acc: 0.0809 - val_loss: 10.3102 - val_acc: 0.0404\n",
      "Epoch 50/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2961 - acc: 0.0735 - val_loss: 10.3093 - val_acc: 0.0404\n",
      "Epoch 51/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2947 - acc: 0.0711 - val_loss: 10.3084 - val_acc: 0.0404\n",
      "Epoch 52/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2936 - acc: 0.0748 - val_loss: 10.3077 - val_acc: 0.0404\n",
      "Epoch 53/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2924 - acc: 0.0723 - val_loss: 10.3069 - val_acc: 0.0404\n",
      "Epoch 54/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2916 - acc: 0.0748 - val_loss: 10.3060 - val_acc: 0.0404\n",
      "Epoch 55/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2903 - acc: 0.0723 - val_loss: 10.3051 - val_acc: 0.0404\n",
      "Epoch 56/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2892 - acc: 0.0748 - val_loss: 10.3043 - val_acc: 0.0404\n",
      "Epoch 57/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2883 - acc: 0.0662 - val_loss: 10.3035 - val_acc: 0.0404\n",
      "Epoch 58/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2870 - acc: 0.0723 - val_loss: 10.3026 - val_acc: 0.0404\n",
      "Epoch 59/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2859 - acc: 0.0699 - val_loss: 10.3018 - val_acc: 0.0404\n",
      "Epoch 60/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2852 - acc: 0.0748 - val_loss: 10.3009 - val_acc: 0.0404\n",
      "Epoch 61/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2836 - acc: 0.0760 - val_loss: 10.2999 - val_acc: 0.0404\n",
      "Epoch 62/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2827 - acc: 0.0760 - val_loss: 10.2990 - val_acc: 0.0404\n",
      "Epoch 63/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2811 - acc: 0.0760 - val_loss: 10.2981 - val_acc: 0.0404\n",
      "Epoch 64/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2805 - acc: 0.0748 - val_loss: 10.2972 - val_acc: 0.0404\n",
      "Epoch 65/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2794 - acc: 0.0699 - val_loss: 10.2963 - val_acc: 0.0404\n",
      "Epoch 66/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2776 - acc: 0.0711 - val_loss: 10.2954 - val_acc: 0.0404\n",
      "Epoch 67/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2775 - acc: 0.0711 - val_loss: 10.2945 - val_acc: 0.0404\n",
      "Epoch 68/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2761 - acc: 0.0735 - val_loss: 10.2935 - val_acc: 0.0404\n",
      "Epoch 69/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2746 - acc: 0.0760 - val_loss: 10.2926 - val_acc: 0.0404\n",
      "Epoch 70/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2738 - acc: 0.0711 - val_loss: 10.2917 - val_acc: 0.0404\n",
      "Epoch 71/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2727 - acc: 0.0723 - val_loss: 10.2909 - val_acc: 0.0404\n",
      "Epoch 72/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2717 - acc: 0.0723 - val_loss: 10.2899 - val_acc: 0.0404\n",
      "Epoch 73/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2708 - acc: 0.0784 - val_loss: 10.2890 - val_acc: 0.0404\n",
      "Epoch 74/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2695 - acc: 0.0723 - val_loss: 10.2881 - val_acc: 0.0404\n",
      "Epoch 75/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2684 - acc: 0.0772 - val_loss: 10.2871 - val_acc: 0.0404\n",
      "Epoch 76/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2670 - acc: 0.0711 - val_loss: 10.2863 - val_acc: 0.0404\n",
      "Epoch 77/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2658 - acc: 0.0699 - val_loss: 10.2853 - val_acc: 0.0404\n",
      "Epoch 78/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2652 - acc: 0.0711 - val_loss: 10.2845 - val_acc: 0.0404\n",
      "Epoch 79/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2637 - acc: 0.0748 - val_loss: 10.2836 - val_acc: 0.0404\n",
      "Epoch 80/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2627 - acc: 0.0735 - val_loss: 10.2829 - val_acc: 0.0404\n",
      "Epoch 81/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2615 - acc: 0.0699 - val_loss: 10.2820 - val_acc: 0.0404\n",
      "Epoch 82/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2600 - acc: 0.0699 - val_loss: 10.2810 - val_acc: 0.0404\n",
      "Epoch 83/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2594 - acc: 0.0723 - val_loss: 10.2800 - val_acc: 0.0404\n",
      "Epoch 84/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2582 - acc: 0.0686 - val_loss: 10.2793 - val_acc: 0.0404\n",
      "Epoch 85/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2576 - acc: 0.0686 - val_loss: 10.2782 - val_acc: 0.0404\n",
      "Epoch 86/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2557 - acc: 0.0699 - val_loss: 10.2774 - val_acc: 0.0404\n",
      "Epoch 87/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2554 - acc: 0.0735 - val_loss: 10.2764 - val_acc: 0.0404\n",
      "Epoch 88/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2533 - acc: 0.0711 - val_loss: 10.2755 - val_acc: 0.0404\n",
      "Epoch 89/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2526 - acc: 0.0711 - val_loss: 10.2745 - val_acc: 0.0404\n",
      "Epoch 90/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2511 - acc: 0.0735 - val_loss: 10.2736 - val_acc: 0.0404\n",
      "Epoch 91/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2507 - acc: 0.0711 - val_loss: 10.2726 - val_acc: 0.0404\n",
      "Epoch 92/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2491 - acc: 0.0711 - val_loss: 10.2718 - val_acc: 0.0404\n",
      "Epoch 93/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2484 - acc: 0.0699 - val_loss: 10.2707 - val_acc: 0.0404\n",
      "Epoch 94/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2469 - acc: 0.0699 - val_loss: 10.2698 - val_acc: 0.0404\n",
      "Epoch 95/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2459 - acc: 0.0699 - val_loss: 10.2687 - val_acc: 0.0404\n",
      "Epoch 96/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2448 - acc: 0.0711 - val_loss: 10.2679 - val_acc: 0.0404\n",
      "Epoch 97/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2435 - acc: 0.0711 - val_loss: 10.2668 - val_acc: 0.0404\n",
      "Epoch 98/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2424 - acc: 0.0711 - val_loss: 10.2657 - val_acc: 0.0404\n",
      "Epoch 99/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2407 - acc: 0.0711 - val_loss: 10.2647 - val_acc: 0.0404\n",
      "Epoch 100/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2406 - acc: 0.0711 - val_loss: 10.2636 - val_acc: 0.0404\n",
      "Epoch 101/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2386 - acc: 0.0723 - val_loss: 10.2627 - val_acc: 0.0404\n",
      "Epoch 102/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2384 - acc: 0.0699 - val_loss: 10.2616 - val_acc: 0.0404\n",
      "Epoch 103/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2368 - acc: 0.0699 - val_loss: 10.2605 - val_acc: 0.0404\n",
      "Epoch 104/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2358 - acc: 0.0699 - val_loss: 10.2596 - val_acc: 0.0404\n",
      "Epoch 105/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2341 - acc: 0.0748 - val_loss: 10.2584 - val_acc: 0.0404\n",
      "Epoch 106/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2331 - acc: 0.0711 - val_loss: 10.2573 - val_acc: 0.0404\n",
      "Epoch 107/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2319 - acc: 0.0711 - val_loss: 10.2563 - val_acc: 0.0404\n",
      "Epoch 108/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2308 - acc: 0.0699 - val_loss: 10.2552 - val_acc: 0.0404\n",
      "Epoch 109/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2299 - acc: 0.0711 - val_loss: 10.2542 - val_acc: 0.0404\n",
      "Epoch 110/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2284 - acc: 0.0735 - val_loss: 10.2532 - val_acc: 0.0404\n",
      "Epoch 111/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2274 - acc: 0.0699 - val_loss: 10.2520 - val_acc: 0.0404\n",
      "Epoch 112/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2261 - acc: 0.0699 - val_loss: 10.2507 - val_acc: 0.0404\n",
      "Epoch 113/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2254 - acc: 0.0711 - val_loss: 10.2496 - val_acc: 0.0404\n",
      "Epoch 114/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2235 - acc: 0.0699 - val_loss: 10.2484 - val_acc: 0.0404\n",
      "Epoch 115/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2226 - acc: 0.0711 - val_loss: 10.2473 - val_acc: 0.0404\n",
      "Epoch 116/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2214 - acc: 0.0723 - val_loss: 10.2461 - val_acc: 0.0404\n",
      "Epoch 117/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2198 - acc: 0.0723 - val_loss: 10.2451 - val_acc: 0.0404\n",
      "Epoch 118/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2186 - acc: 0.0711 - val_loss: 10.2437 - val_acc: 0.0404\n",
      "Epoch 119/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2174 - acc: 0.0699 - val_loss: 10.2423 - val_acc: 0.0404\n",
      "Epoch 120/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2152 - acc: 0.0723 - val_loss: 10.2411 - val_acc: 0.0404\n",
      "Epoch 121/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2139 - acc: 0.0735 - val_loss: 10.2398 - val_acc: 0.0404\n",
      "Epoch 122/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2122 - acc: 0.0735 - val_loss: 10.2385 - val_acc: 0.0404\n",
      "Epoch 123/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2114 - acc: 0.0748 - val_loss: 10.2371 - val_acc: 0.0404\n",
      "Epoch 124/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2100 - acc: 0.0748 - val_loss: 10.2354 - val_acc: 0.0404\n",
      "Epoch 125/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2095 - acc: 0.0735 - val_loss: 10.2343 - val_acc: 0.0404\n",
      "Epoch 126/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2074 - acc: 0.0699 - val_loss: 10.2327 - val_acc: 0.0404\n",
      "Epoch 127/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.2061 - acc: 0.0723 - val_loss: 10.2312 - val_acc: 0.0404\n",
      "Epoch 128/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2045 - acc: 0.0748 - val_loss: 10.2293 - val_acc: 0.0404\n",
      "Epoch 129/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.2027 - acc: 0.0760 - val_loss: 10.2275 - val_acc: 0.0404\n",
      "Epoch 130/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.2005 - acc: 0.0748 - val_loss: 10.2263 - val_acc: 0.0404\n",
      "Epoch 131/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.1997 - acc: 0.0748 - val_loss: 10.2243 - val_acc: 0.0404\n",
      "Epoch 132/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.1985 - acc: 0.0760 - val_loss: 10.2223 - val_acc: 0.0404\n",
      "Epoch 133/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.1965 - acc: 0.0760 - val_loss: 10.2203 - val_acc: 0.0441\n",
      "Epoch 134/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.1942 - acc: 0.0760 - val_loss: 10.2182 - val_acc: 0.0441\n",
      "Epoch 135/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.1922 - acc: 0.0797 - val_loss: 10.2162 - val_acc: 0.0441\n",
      "Epoch 136/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.1902 - acc: 0.0797 - val_loss: 10.2144 - val_acc: 0.0441\n",
      "Epoch 137/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.1882 - acc: 0.0846 - val_loss: 10.2114 - val_acc: 0.0441\n",
      "Epoch 138/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.1856 - acc: 0.0858 - val_loss: 10.2086 - val_acc: 0.0515\n",
      "Epoch 139/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.1833 - acc: 0.0882 - val_loss: 10.2054 - val_acc: 0.0735\n",
      "Epoch 140/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.1811 - acc: 0.0735 - val_loss: 10.2034 - val_acc: 0.0515\n",
      "Epoch 141/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.1777 - acc: 0.0870 - val_loss: 10.1985 - val_acc: 0.0809\n",
      "Epoch 142/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 10.1730 - acc: 0.0919 - val_loss: 10.1964 - val_acc: 0.0625\n",
      "Epoch 143/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.1711 - acc: 0.0919 - val_loss: 10.1900 - val_acc: 0.0772\n",
      "Epoch 144/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.1658 - acc: 0.0980 - val_loss: 10.1849 - val_acc: 0.0772\n",
      "Epoch 145/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.1590 - acc: 0.1066 - val_loss: 10.1773 - val_acc: 0.0846\n",
      "Epoch 146/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.1531 - acc: 0.1103 - val_loss: 10.1702 - val_acc: 0.0772\n",
      "Epoch 147/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.1467 - acc: 0.1213 - val_loss: 10.1609 - val_acc: 0.1029\n",
      "Epoch 148/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.1363 - acc: 0.1360 - val_loss: 10.1419 - val_acc: 0.1140\n",
      "Epoch 149/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.1232 - acc: 0.1385 - val_loss: 10.1210 - val_acc: 0.1140\n",
      "Epoch 150/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.1010 - acc: 0.1311 - val_loss: 10.0954 - val_acc: 0.1360\n",
      "Epoch 151/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.0869 - acc: 0.1422 - val_loss: 10.0720 - val_acc: 0.1140\n",
      "Epoch 152/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.0658 - acc: 0.1103 - val_loss: 10.0634 - val_acc: 0.1066\n",
      "Epoch 153/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 10.0674 - acc: 0.1238 - val_loss: 10.0084 - val_acc: 0.1066\n",
      "Epoch 154/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 10.0429 - acc: 0.1238 - val_loss: 9.9358 - val_acc: 0.1397\n",
      "Epoch 155/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.9239 - acc: 0.1385 - val_loss: 9.9397 - val_acc: 0.1250\n",
      "Epoch 156/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.9948 - acc: 0.1176 - val_loss: 9.8927 - val_acc: 0.1287\n",
      "Epoch 157/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.8576 - acc: 0.1581 - val_loss: 9.7772 - val_acc: 0.1654\n",
      "Epoch 158/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.8841 - acc: 0.1275 - val_loss: 9.8810 - val_acc: 0.1728\n",
      "Epoch 159/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.7710 - acc: 0.1703 - val_loss: 9.6989 - val_acc: 0.1875\n",
      "Epoch 160/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.9448 - acc: 0.1360 - val_loss: 9.8077 - val_acc: 0.1581\n",
      "Epoch 161/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.7547 - acc: 0.1850 - val_loss: 9.7113 - val_acc: 0.1691\n",
      "Epoch 162/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.7057 - acc: 0.1801 - val_loss: 9.7211 - val_acc: 0.2096\n",
      "Epoch 163/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.7536 - acc: 0.1728 - val_loss: 9.6705 - val_acc: 0.1949\n",
      "Epoch 164/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.6389 - acc: 0.2047 - val_loss: 9.7780 - val_acc: 0.1728\n",
      "Epoch 165/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.8034 - acc: 0.1814 - val_loss: 9.6308 - val_acc: 0.1618\n",
      "Epoch 166/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.6106 - acc: 0.2206 - val_loss: 9.6743 - val_acc: 0.1287\n",
      "Epoch 167/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.5867 - acc: 0.2047 - val_loss: 9.5768 - val_acc: 0.2279\n",
      "Epoch 168/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 9.5672 - acc: 0.2341 - val_loss: 9.5998 - val_acc: 0.2537\n",
      "Epoch 169/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 9.6808 - acc: 0.2218 - val_loss: 9.6082 - val_acc: 0.1691\n",
      "Epoch 170/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.5319 - acc: 0.2377 - val_loss: 9.6431 - val_acc: 0.2132\n",
      "Epoch 171/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 9.5304 - acc: 0.2463 - val_loss: 9.4983 - val_acc: 0.2132\n",
      "Epoch 172/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.4310 - acc: 0.2806 - val_loss: 9.5541 - val_acc: 0.2500\n",
      "Epoch 173/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 9.4020 - acc: 0.2966 - val_loss: 9.5979 - val_acc: 0.2022\n",
      "Epoch 174/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.4532 - acc: 0.2806 - val_loss: 9.4926 - val_acc: 0.2096\n",
      "Epoch 175/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.4176 - acc: 0.2892 - val_loss: 9.4785 - val_acc: 0.2500\n",
      "Epoch 176/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.3961 - acc: 0.2843 - val_loss: 9.6120 - val_acc: 0.2206\n",
      "Epoch 177/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.6284 - acc: 0.2426 - val_loss: 9.4393 - val_acc: 0.2353\n",
      "Epoch 178/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.3321 - acc: 0.3186 - val_loss: 9.5045 - val_acc: 0.2316\n",
      "Epoch 179/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.2868 - acc: 0.3272 - val_loss: 9.6178 - val_acc: 0.2426\n",
      "Epoch 180/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.4810 - acc: 0.2953 - val_loss: 9.4295 - val_acc: 0.2537\n",
      "Epoch 181/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.3070 - acc: 0.3260 - val_loss: 9.3656 - val_acc: 0.2537\n",
      "Epoch 182/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.2727 - acc: 0.3113 - val_loss: 9.5529 - val_acc: 0.2390\n",
      "Epoch 183/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.2597 - acc: 0.3370 - val_loss: 9.4173 - val_acc: 0.2169\n",
      "Epoch 184/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 9.2352 - acc: 0.3346 - val_loss: 9.4351 - val_acc: 0.2794\n",
      "Epoch 185/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.2354 - acc: 0.3358 - val_loss: 9.3741 - val_acc: 0.2610\n",
      "Epoch 186/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.2226 - acc: 0.3297 - val_loss: 9.3767 - val_acc: 0.2390\n",
      "Epoch 187/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.2245 - acc: 0.3370 - val_loss: 9.3770 - val_acc: 0.2463\n",
      "Epoch 188/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.1803 - acc: 0.3505 - val_loss: 9.3730 - val_acc: 0.2684\n",
      "Epoch 189/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.1472 - acc: 0.3615 - val_loss: 9.5281 - val_acc: 0.2757\n",
      "Epoch 190/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.1445 - acc: 0.3775 - val_loss: 9.3310 - val_acc: 0.2610\n",
      "Epoch 191/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.1724 - acc: 0.3493 - val_loss: 9.3180 - val_acc: 0.2684\n",
      "Epoch 192/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.1426 - acc: 0.3591 - val_loss: 9.3782 - val_acc: 0.2684\n",
      "Epoch 193/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.0530 - acc: 0.3811 - val_loss: 9.5759 - val_acc: 0.2500\n",
      "Epoch 194/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 9.2002 - acc: 0.3591 - val_loss: 9.3460 - val_acc: 0.2978\n",
      "Epoch 195/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.0904 - acc: 0.3762 - val_loss: 9.3449 - val_acc: 0.2684\n",
      "Epoch 196/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.0943 - acc: 0.3676 - val_loss: 9.5646 - val_acc: 0.2243\n",
      "Epoch 197/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.1114 - acc: 0.3934 - val_loss: 9.3260 - val_acc: 0.3015\n",
      "Epoch 198/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.0332 - acc: 0.4069 - val_loss: 9.3582 - val_acc: 0.2574\n",
      "Epoch 199/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 9.0380 - acc: 0.4044 - val_loss: 9.4192 - val_acc: 0.2721\n",
      "Epoch 200/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 9.0176 - acc: 0.3995 - val_loss: 9.3594 - val_acc: 0.2794\n",
      "Epoch 201/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.0013 - acc: 0.4130 - val_loss: 9.6717 - val_acc: 0.2537\n",
      "Epoch 202/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 9.2764 - acc: 0.3260 - val_loss: 9.2692 - val_acc: 0.2647\n",
      "Epoch 203/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.9610 - acc: 0.4203 - val_loss: 9.2845 - val_acc: 0.3162\n",
      "Epoch 204/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.9676 - acc: 0.4216 - val_loss: 9.3113 - val_acc: 0.2978\n",
      "Epoch 205/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.8997 - acc: 0.4363 - val_loss: 9.2820 - val_acc: 0.2941\n",
      "Epoch 206/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.9615 - acc: 0.4387 - val_loss: 9.4071 - val_acc: 0.2794\n",
      "Epoch 207/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.9735 - acc: 0.4289 - val_loss: 9.2585 - val_acc: 0.2794\n",
      "Epoch 208/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.9749 - acc: 0.4277 - val_loss: 9.3852 - val_acc: 0.2684\n",
      "Epoch 209/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 8.9328 - acc: 0.4240 - val_loss: 9.2085 - val_acc: 0.3051\n",
      "Epoch 210/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.8771 - acc: 0.4547 - val_loss: 9.1758 - val_acc: 0.3493\n",
      "Epoch 211/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.8269 - acc: 0.4412 - val_loss: 9.6169 - val_acc: 0.3015\n",
      "Epoch 212/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 9.0359 - acc: 0.4167 - val_loss: 9.3881 - val_acc: 0.3235\n",
      "Epoch 213/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.8455 - acc: 0.4706 - val_loss: 9.3521 - val_acc: 0.3015\n",
      "Epoch 214/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.7908 - acc: 0.4743 - val_loss: 9.1865 - val_acc: 0.3493\n",
      "Epoch 215/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 8.7748 - acc: 0.4694 - val_loss: 9.5111 - val_acc: 0.2831\n",
      "Epoch 216/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.9359 - acc: 0.4522 - val_loss: 9.4014 - val_acc: 0.3309\n",
      "Epoch 217/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.8197 - acc: 0.4951 - val_loss: 9.1904 - val_acc: 0.3088\n",
      "Epoch 218/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 8.7522 - acc: 0.4779 - val_loss: 9.2808 - val_acc: 0.3235\n",
      "Epoch 219/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 8.8011 - acc: 0.4706 - val_loss: 9.4067 - val_acc: 0.2610\n",
      "Epoch 220/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 8.7118 - acc: 0.5257 - val_loss: 9.2405 - val_acc: 0.3051\n",
      "Epoch 221/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 8.7349 - acc: 0.4865 - val_loss: 9.3037 - val_acc: 0.3235\n",
      "Epoch 222/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.6979 - acc: 0.5196 - val_loss: 9.5619 - val_acc: 0.2610\n",
      "Epoch 223/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.6875 - acc: 0.5331 - val_loss: 9.4176 - val_acc: 0.3272\n",
      "Epoch 224/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.6869 - acc: 0.5037 - val_loss: 10.1434 - val_acc: 0.2684\n",
      "Epoch 225/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.8329 - acc: 0.4914 - val_loss: 9.4526 - val_acc: 0.3529\n",
      "Epoch 226/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 8.6425 - acc: 0.5245 - val_loss: 9.1620 - val_acc: 0.3456\n",
      "Epoch 227/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.6102 - acc: 0.5417 - val_loss: 9.3376 - val_acc: 0.3346\n",
      "Epoch 228/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.7230 - acc: 0.5012 - val_loss: 9.2311 - val_acc: 0.3272\n",
      "Epoch 229/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.5862 - acc: 0.5380 - val_loss: 9.3976 - val_acc: 0.3088\n",
      "Epoch 230/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.6042 - acc: 0.5417 - val_loss: 9.3376 - val_acc: 0.2794\n",
      "Epoch 231/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.5819 - acc: 0.5527 - val_loss: 9.3403 - val_acc: 0.3051\n",
      "Epoch 232/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.5789 - acc: 0.5417 - val_loss: 9.1782 - val_acc: 0.3419\n",
      "Epoch 233/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.4992 - acc: 0.5760 - val_loss: 9.4212 - val_acc: 0.3272\n",
      "Epoch 234/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.5268 - acc: 0.5625 - val_loss: 9.2105 - val_acc: 0.3162\n",
      "Epoch 235/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.4804 - acc: 0.5809 - val_loss: 9.2788 - val_acc: 0.3824\n",
      "Epoch 236/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.4039 - acc: 0.6140 - val_loss: 9.1895 - val_acc: 0.3676\n",
      "Epoch 237/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.6869 - acc: 0.5331 - val_loss: 9.3841 - val_acc: 0.3566\n",
      "Epoch 238/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.4928 - acc: 0.5772 - val_loss: 9.3553 - val_acc: 0.3346\n",
      "Epoch 239/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.4750 - acc: 0.5882 - val_loss: 9.6634 - val_acc: 0.3824\n",
      "Epoch 240/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.5457 - acc: 0.5723 - val_loss: 9.2708 - val_acc: 0.3971\n",
      "Epoch 241/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 8.3905 - acc: 0.6115 - val_loss: 9.5021 - val_acc: 0.3603\n",
      "Epoch 242/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.4037 - acc: 0.6091 - val_loss: 9.2495 - val_acc: 0.3750\n",
      "Epoch 243/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.3376 - acc: 0.6299 - val_loss: 9.1738 - val_acc: 0.3971\n",
      "Epoch 244/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.3386 - acc: 0.6385 - val_loss: 9.4068 - val_acc: 0.3603\n",
      "Epoch 245/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.3659 - acc: 0.6262 - val_loss: 9.4122 - val_acc: 0.3640\n",
      "Epoch 246/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 8.4053 - acc: 0.6029 - val_loss: 9.3789 - val_acc: 0.4007\n",
      "Epoch 247/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.2738 - acc: 0.6446 - val_loss: 9.3849 - val_acc: 0.3934\n",
      "Epoch 248/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.3036 - acc: 0.6520 - val_loss: 9.2506 - val_acc: 0.4007\n",
      "Epoch 249/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.2966 - acc: 0.6299 - val_loss: 9.2378 - val_acc: 0.4118\n",
      "Epoch 250/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.1385 - acc: 0.6924 - val_loss: 9.5231 - val_acc: 0.3272\n",
      "Epoch 251/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.3620 - acc: 0.6299 - val_loss: 9.1718 - val_acc: 0.3934\n",
      "Epoch 252/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.2050 - acc: 0.6691 - val_loss: 9.3538 - val_acc: 0.4081\n",
      "Epoch 253/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.1249 - acc: 0.7059 - val_loss: 9.5733 - val_acc: 0.3493\n",
      "Epoch 254/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.3413 - acc: 0.6863 - val_loss: 9.2176 - val_acc: 0.4228\n",
      "Epoch 255/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 8.1063 - acc: 0.7034 - val_loss: 9.3846 - val_acc: 0.4154\n",
      "Epoch 256/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 8.0840 - acc: 0.7255 - val_loss: 9.3158 - val_acc: 0.4375\n",
      "Epoch 257/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 8.0464 - acc: 0.7279 - val_loss: 9.5572 - val_acc: 0.4228\n",
      "Epoch 258/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.1561 - acc: 0.6924 - val_loss: 9.4286 - val_acc: 0.4154\n",
      "Epoch 259/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.0438 - acc: 0.7267 - val_loss: 9.3314 - val_acc: 0.4228\n",
      "Epoch 260/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.0318 - acc: 0.7218 - val_loss: 9.7678 - val_acc: 0.3640\n",
      "Epoch 261/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.1764 - acc: 0.6998 - val_loss: 9.3419 - val_acc: 0.4449\n",
      "Epoch 262/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.0224 - acc: 0.7132 - val_loss: 9.5709 - val_acc: 0.3566\n",
      "Epoch 263/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 8.0213 - acc: 0.7439 - val_loss: 9.1655 - val_acc: 0.4375\n",
      "Epoch 264/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.0715 - acc: 0.7341 - val_loss: 9.4666 - val_acc: 0.3787\n",
      "Epoch 265/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.9509 - acc: 0.7672 - val_loss: 9.3271 - val_acc: 0.4375\n",
      "Epoch 266/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.8424 - acc: 0.7880 - val_loss: 9.3980 - val_acc: 0.4412\n",
      "Epoch 267/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 8.1931 - acc: 0.6850 - val_loss: 9.0534 - val_acc: 0.4485\n",
      "Epoch 268/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.8157 - acc: 0.8150 - val_loss: 9.4162 - val_acc: 0.4485\n",
      "Epoch 269/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.8151 - acc: 0.8150 - val_loss: 9.3817 - val_acc: 0.4338\n",
      "Epoch 270/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.7748 - acc: 0.8100 - val_loss: 9.5933 - val_acc: 0.3860\n",
      "Epoch 271/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.9185 - acc: 0.7598 - val_loss: 9.5809 - val_acc: 0.3934\n",
      "Epoch 272/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.6872 - acc: 0.8517 - val_loss: 9.6214 - val_acc: 0.4485\n",
      "Epoch 273/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.7984 - acc: 0.8125 - val_loss: 9.3876 - val_acc: 0.4706\n",
      "Epoch 274/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.6977 - acc: 0.8493 - val_loss: 10.3841 - val_acc: 0.3603\n",
      "Epoch 275/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 8.3821 - acc: 0.6900 - val_loss: 9.4737 - val_acc: 0.3971\n",
      "Epoch 276/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.7341 - acc: 0.8480 - val_loss: 9.7640 - val_acc: 0.4301\n",
      "Epoch 277/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.7719 - acc: 0.8235 - val_loss: 9.6462 - val_acc: 0.4301\n",
      "Epoch 278/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.7412 - acc: 0.8260 - val_loss: 10.0050 - val_acc: 0.3640\n",
      "Epoch 279/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.8140 - acc: 0.8100 - val_loss: 9.5855 - val_acc: 0.4743\n",
      "Epoch 280/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.5747 - acc: 0.8787 - val_loss: 9.8820 - val_acc: 0.4890\n",
      "Epoch 281/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.5429 - acc: 0.8873 - val_loss: 9.7915 - val_acc: 0.4632\n",
      "Epoch 282/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.5500 - acc: 0.8885 - val_loss: 10.0676 - val_acc: 0.4743\n",
      "Epoch 283/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.6969 - acc: 0.8615 - val_loss: 9.8121 - val_acc: 0.4485\n",
      "Epoch 284/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.4901 - acc: 0.9044 - val_loss: 10.0952 - val_acc: 0.4485\n",
      "Epoch 285/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.5470 - acc: 0.8848 - val_loss: 10.4391 - val_acc: 0.4007\n",
      "Epoch 286/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.6336 - acc: 0.8873 - val_loss: 9.7723 - val_acc: 0.4338\n",
      "Epoch 287/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.4900 - acc: 0.9105 - val_loss: 9.9022 - val_acc: 0.4632\n",
      "Epoch 288/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.9272 - acc: 0.7757 - val_loss: 9.4322 - val_acc: 0.4228\n",
      "Epoch 289/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.6334 - acc: 0.8701 - val_loss: 9.5731 - val_acc: 0.4853\n",
      "Epoch 290/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.4168 - acc: 0.9363 - val_loss: 9.9364 - val_acc: 0.4853\n",
      "Epoch 291/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.4292 - acc: 0.9289 - val_loss: 10.3372 - val_acc: 0.4007\n",
      "Epoch 292/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.4875 - acc: 0.9130 - val_loss: 10.0935 - val_acc: 0.4632\n",
      "Epoch 293/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.4819 - acc: 0.9203 - val_loss: 9.8326 - val_acc: 0.4816\n",
      "Epoch 294/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.3830 - acc: 0.9449 - val_loss: 10.3950 - val_acc: 0.4228\n",
      "Epoch 295/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.4784 - acc: 0.9179 - val_loss: 10.0007 - val_acc: 0.4743\n",
      "Epoch 296/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.7787 - acc: 0.8137 - val_loss: 10.1355 - val_acc: 0.3640\n",
      "Epoch 297/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.6924 - acc: 0.8640 - val_loss: 9.7645 - val_acc: 0.4706\n",
      "Epoch 298/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.4425 - acc: 0.9252 - val_loss: 9.9562 - val_acc: 0.4963\n",
      "Epoch 299/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.3723 - acc: 0.9559 - val_loss: 10.2512 - val_acc: 0.4706\n",
      "Epoch 300/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.3634 - acc: 0.9559 - val_loss: 10.8808 - val_acc: 0.4228\n",
      "Epoch 301/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.4861 - acc: 0.9191 - val_loss: 10.0156 - val_acc: 0.4853\n",
      "Epoch 302/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.3547 - acc: 0.9608 - val_loss: 10.2571 - val_acc: 0.4596\n",
      "Epoch 303/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.3612 - acc: 0.9596 - val_loss: 10.2239 - val_acc: 0.4853\n",
      "Epoch 304/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.3656 - acc: 0.9534 - val_loss: 10.0417 - val_acc: 0.4743\n",
      "Epoch 305/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.3596 - acc: 0.9534 - val_loss: 10.2163 - val_acc: 0.4779\n",
      "Epoch 306/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.3546 - acc: 0.9510 - val_loss: 10.2330 - val_acc: 0.4853\n",
      "Epoch 307/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.3767 - acc: 0.9485 - val_loss: 10.1942 - val_acc: 0.4559\n",
      "Epoch 308/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.3413 - acc: 0.9547 - val_loss: 10.0856 - val_acc: 0.4816\n",
      "Epoch 309/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.3303 - acc: 0.9571 - val_loss: 10.1889 - val_acc: 0.4890\n",
      "Epoch 310/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.3635 - acc: 0.9473 - val_loss: 10.2831 - val_acc: 0.4963\n",
      "Epoch 311/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.3535 - acc: 0.9534 - val_loss: 10.1125 - val_acc: 0.4779\n",
      "Epoch 312/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.3245 - acc: 0.9559 - val_loss: 10.1784 - val_acc: 0.4522\n",
      "Epoch 313/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.3285 - acc: 0.9596 - val_loss: 10.4515 - val_acc: 0.4743\n",
      "Epoch 314/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.3231 - acc: 0.9730 - val_loss: 10.5388 - val_acc: 0.4706\n",
      "Epoch 315/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2862 - acc: 0.9743 - val_loss: 10.4571 - val_acc: 0.4596\n",
      "Epoch 316/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.2988 - acc: 0.9694 - val_loss: 10.6861 - val_acc: 0.4706\n",
      "Epoch 317/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.4148 - acc: 0.9363 - val_loss: 10.3054 - val_acc: 0.4779\n",
      "Epoch 318/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.3068 - acc: 0.9645 - val_loss: 10.3898 - val_acc: 0.4890\n",
      "Epoch 319/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.4070 - acc: 0.9412 - val_loss: 10.7379 - val_acc: 0.4412\n",
      "Epoch 320/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.3755 - acc: 0.9510 - val_loss: 10.0874 - val_acc: 0.5110\n",
      "Epoch 321/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.2758 - acc: 0.9767 - val_loss: 10.3838 - val_acc: 0.4963\n",
      "Epoch 322/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2604 - acc: 0.9779 - val_loss: 10.4758 - val_acc: 0.4743\n",
      "Epoch 323/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2613 - acc: 0.9792 - val_loss: 10.5319 - val_acc: 0.4890\n",
      "Epoch 324/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2669 - acc: 0.9804 - val_loss: 10.7710 - val_acc: 0.5037\n",
      "Epoch 325/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.6945 - acc: 0.8505 - val_loss: 9.6860 - val_acc: 0.5037\n",
      "Epoch 326/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.2780 - acc: 0.9730 - val_loss: 10.2207 - val_acc: 0.5037\n",
      "Epoch 327/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2595 - acc: 0.9792 - val_loss: 10.3764 - val_acc: 0.4779\n",
      "Epoch 328/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.2304 - acc: 0.9877 - val_loss: 10.7008 - val_acc: 0.4963\n",
      "Epoch 329/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2500 - acc: 0.9865 - val_loss: 10.8852 - val_acc: 0.4338\n",
      "Epoch 330/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2851 - acc: 0.9730 - val_loss: 10.7227 - val_acc: 0.4890\n",
      "Epoch 331/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2833 - acc: 0.9755 - val_loss: 10.4862 - val_acc: 0.5074\n",
      "Epoch 332/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2334 - acc: 0.9865 - val_loss: 10.6994 - val_acc: 0.5037\n",
      "Epoch 333/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2439 - acc: 0.9853 - val_loss: 10.8075 - val_acc: 0.4926\n",
      "Epoch 334/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.2425 - acc: 0.9841 - val_loss: 10.6103 - val_acc: 0.4816\n",
      "Epoch 335/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2554 - acc: 0.9755 - val_loss: 10.6021 - val_acc: 0.4743\n",
      "Epoch 336/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2866 - acc: 0.9694 - val_loss: 10.5748 - val_acc: 0.4706\n",
      "Epoch 337/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2449 - acc: 0.9816 - val_loss: 10.7023 - val_acc: 0.5074\n",
      "Epoch 338/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2471 - acc: 0.9755 - val_loss: 10.5164 - val_acc: 0.4963\n",
      "Epoch 339/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2493 - acc: 0.9792 - val_loss: 10.7045 - val_acc: 0.5147\n",
      "Epoch 340/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2480 - acc: 0.9755 - val_loss: 10.5898 - val_acc: 0.5074\n",
      "Epoch 341/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2174 - acc: 0.9877 - val_loss: 10.6849 - val_acc: 0.5000\n",
      "Epoch 342/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2180 - acc: 0.9877 - val_loss: 10.7702 - val_acc: 0.5074\n",
      "Epoch 343/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2152 - acc: 0.9890 - val_loss: 10.8072 - val_acc: 0.4853\n",
      "Epoch 344/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2327 - acc: 0.9792 - val_loss: 10.8918 - val_acc: 0.5110\n",
      "Epoch 345/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.3768 - acc: 0.9412 - val_loss: 10.4142 - val_acc: 0.4669\n",
      "Epoch 346/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2242 - acc: 0.9828 - val_loss: 10.6630 - val_acc: 0.4485\n",
      "Epoch 347/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2580 - acc: 0.9718 - val_loss: 10.3295 - val_acc: 0.4926\n",
      "Epoch 348/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2231 - acc: 0.9865 - val_loss: 10.4726 - val_acc: 0.5037\n",
      "Epoch 349/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2020 - acc: 0.9939 - val_loss: 10.7617 - val_acc: 0.5184\n",
      "Epoch 350/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2118 - acc: 0.9841 - val_loss: 10.7354 - val_acc: 0.4890\n",
      "Epoch 351/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2001 - acc: 0.9926 - val_loss: 10.7944 - val_acc: 0.5037\n",
      "Epoch 352/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1984 - acc: 0.9914 - val_loss: 10.9176 - val_acc: 0.4963\n",
      "Epoch 353/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1994 - acc: 0.9939 - val_loss: 10.8612 - val_acc: 0.5147\n",
      "Epoch 354/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2186 - acc: 0.9853 - val_loss: 10.7478 - val_acc: 0.5147\n",
      "Epoch 355/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1983 - acc: 0.9914 - val_loss: 10.9655 - val_acc: 0.4706\n",
      "Epoch 356/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1926 - acc: 0.9914 - val_loss: 11.0106 - val_acc: 0.5000\n",
      "Epoch 357/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.2331 - acc: 0.9779 - val_loss: 10.9213 - val_acc: 0.5404\n",
      "Epoch 358/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2088 - acc: 0.9865 - val_loss: 10.8121 - val_acc: 0.5221\n",
      "Epoch 359/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2055 - acc: 0.9914 - val_loss: 10.7832 - val_acc: 0.5000\n",
      "Epoch 360/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1985 - acc: 0.9877 - val_loss: 10.8119 - val_acc: 0.4926\n",
      "Epoch 361/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2018 - acc: 0.9902 - val_loss: 10.9394 - val_acc: 0.4853\n",
      "Epoch 362/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2085 - acc: 0.9804 - val_loss: 10.8663 - val_acc: 0.4890\n",
      "Epoch 363/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2023 - acc: 0.9853 - val_loss: 10.8984 - val_acc: 0.4890\n",
      "Epoch 364/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2179 - acc: 0.9804 - val_loss: 11.1668 - val_acc: 0.4449\n",
      "Epoch 365/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.2677 - acc: 0.9608 - val_loss: 10.9326 - val_acc: 0.4669\n",
      "Epoch 366/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2671 - acc: 0.9681 - val_loss: 10.6575 - val_acc: 0.4816\n",
      "Epoch 367/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.2291 - acc: 0.9767 - val_loss: 10.4207 - val_acc: 0.4926\n",
      "Epoch 368/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1808 - acc: 0.9939 - val_loss: 10.5933 - val_acc: 0.4853\n",
      "Epoch 369/400\n",
      "816/816 [==============================] - 13s 17ms/step - loss: 7.2079 - acc: 0.9792 - val_loss: 10.5528 - val_acc: 0.4926\n",
      "Epoch 370/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1839 - acc: 0.9939 - val_loss: 10.6748 - val_acc: 0.4963\n",
      "Epoch 371/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.1855 - acc: 0.9853 - val_loss: 10.8127 - val_acc: 0.4890\n",
      "Epoch 372/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2026 - acc: 0.9865 - val_loss: 11.1052 - val_acc: 0.4816\n",
      "Epoch 373/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.4991 - acc: 0.9105 - val_loss: 9.9887 - val_acc: 0.4926\n",
      "Epoch 374/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.2139 - acc: 0.9804 - val_loss: 10.2261 - val_acc: 0.5184\n",
      "Epoch 375/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.1849 - acc: 0.9926 - val_loss: 10.4043 - val_acc: 0.5184\n",
      "Epoch 376/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1691 - acc: 0.9988 - val_loss: 10.4615 - val_acc: 0.5184\n",
      "Epoch 377/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1678 - acc: 0.9926 - val_loss: 10.5673 - val_acc: 0.5331\n",
      "Epoch 378/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1792 - acc: 0.9902 - val_loss: 10.7003 - val_acc: 0.5037\n",
      "Epoch 379/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1605 - acc: 0.9988 - val_loss: 10.7423 - val_acc: 0.5184\n",
      "Epoch 380/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1698 - acc: 0.9926 - val_loss: 10.8009 - val_acc: 0.5074\n",
      "Epoch 381/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1726 - acc: 0.9902 - val_loss: 10.8671 - val_acc: 0.4779\n",
      "Epoch 382/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.2073 - acc: 0.9743 - val_loss: 10.8186 - val_acc: 0.5110\n",
      "Epoch 383/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1864 - acc: 0.9828 - val_loss: 10.7396 - val_acc: 0.4890\n",
      "Epoch 384/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1915 - acc: 0.9816 - val_loss: 10.7497 - val_acc: 0.4926\n",
      "Epoch 385/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2258 - acc: 0.9755 - val_loss: 10.6678 - val_acc: 0.5037\n",
      "Epoch 386/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1617 - acc: 0.9975 - val_loss: 10.8185 - val_acc: 0.4926\n",
      "Epoch 387/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1584 - acc: 0.9951 - val_loss: 10.8871 - val_acc: 0.5110\n",
      "Epoch 388/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1699 - acc: 0.9914 - val_loss: 10.8653 - val_acc: 0.5110\n",
      "Epoch 389/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1559 - acc: 0.9975 - val_loss: 10.8987 - val_acc: 0.5147\n",
      "Epoch 390/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1826 - acc: 0.9853 - val_loss: 10.9841 - val_acc: 0.4743\n",
      "Epoch 391/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2249 - acc: 0.9669 - val_loss: 10.5123 - val_acc: 0.5110\n",
      "Epoch 392/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1789 - acc: 0.9865 - val_loss: 10.6230 - val_acc: 0.5074\n",
      "Epoch 393/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1716 - acc: 0.9841 - val_loss: 10.5285 - val_acc: 0.5074\n",
      "Epoch 394/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1502 - acc: 0.9951 - val_loss: 10.5962 - val_acc: 0.5184\n",
      "Epoch 395/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1527 - acc: 0.9951 - val_loss: 11.0087 - val_acc: 0.4963\n",
      "Epoch 396/400\n",
      "816/816 [==============================] - 14s 17ms/step - loss: 7.1537 - acc: 0.9939 - val_loss: 11.0579 - val_acc: 0.5074\n",
      "Epoch 397/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1668 - acc: 0.9877 - val_loss: 11.1384 - val_acc: 0.4926\n",
      "Epoch 398/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1555 - acc: 0.9926 - val_loss: 11.0014 - val_acc: 0.4853\n",
      "Epoch 399/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.1636 - acc: 0.9865 - val_loss: 11.0982 - val_acc: 0.4632\n",
      "Epoch 400/400\n",
      "816/816 [==============================] - 13s 16ms/step - loss: 7.2205 - acc: 0.9681 - val_loss: 10.7139 - val_acc: 0.4963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ac0e11da0>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGD(lr=0.1, momentum=0.9, decay=0.0005)\n",
    "vgg16.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "vgg16.fit(X_train, Y_train, batch_size=128, epochs=400, validation_split=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "vDkaoq9jjEfP",
    "outputId": "9d784ee5-9a49-4438-900c-0218f99c93b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 1s 5ms/step\n",
      "Loss on test set:  10.833361120784984\n",
      "Accuracy on test set:  0.4852941176470588\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = vgg16.evaluate(X_test, Y_test, batch_size=128)\n",
    "print(\"Loss on test set: \", loss_and_metrics[0])\n",
    "print(\"Accuracy on test set: \", loss_and_metrics[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "VGG16.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
